{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bf8e715",
   "metadata": {},
   "source": [
    "# CLEANING AND PREPROCESSING\n",
    "The goal of this phase is to remove inconsistencies, missing values and duplications from the data, preparing it for analysis and modelling.\n",
    "\n",
    "The cleaning process will be performed separately on each dataset as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c858d759",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing relevant libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2cc85b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Setup (paths + helpers)\n",
    "\n",
    "# ---- Paths (edit if needed) ----\n",
    "RAW_DIR     = Path(\"../Data\")\n",
    "CLEAN_DIR   = Path(\"../Data/Cleaned Dataset\")\n",
    "CLEAN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RAW_EVENTS  = RAW_DIR / \"events.csv\"                 # rename if different\n",
    "RAW_CAT     = RAW_DIR / \"category_tree.csv\"\n",
    "RAW_IP1     = RAW_DIR / \"item_properties_part1.1.csv\"  # rename if different\n",
    "RAW_IP2     = RAW_DIR / \"item_properties_part2.csv\"\n",
    "\n",
    "OUT_EVENTS_CSV  = CLEAN_DIR / \"cleaned_user_events.csv\"\n",
    "OUT_EVENTS_PQ   = CLEAN_DIR / \"cleaned_user_events.parquet\"\n",
    "OUT_CAT_CSV     = CLEAN_DIR / \"cleaned_category_tree.csv\"\n",
    "OUT_IP_LATEST_PQ= CLEAN_DIR / \"cleaned_item_properties_latest.parquet\"  # long format, latest per (item,prop)\n",
    "OUT_IP_WIDE_PQ  = CLEAN_DIR / \"cleaned_item_properties_wide.parquet\"    # optional wide format (selected props)\n",
    "\n",
    "# Value cleaning helper: extract first numeric token; keep booleans; else keep string\n",
    "num_pat = re.compile(r\"[-+]?\\d*\\.?\\d+\")\n",
    "def clean_value(v):\n",
    "    if pd.isna(v):\n",
    "        return np.nan\n",
    "    if isinstance(v, (int, float)):\n",
    "        return v\n",
    "    s = str(v).strip().lower()\n",
    "    if s in {\"true\", \"yes\"}:  return 1\n",
    "    if s in {\"false\",\"no\"}:   return 0\n",
    "    m = num_pat.search(s.lstrip(\"n\"))  # drop a leading 'n' then find number\n",
    "    return float(m.group()) if m else s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2001a1",
   "metadata": {},
   "source": [
    "1. Cleaning the category_tree dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be7ce5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Cleaned Category Tree ==\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1644 entries, 0 to 1668\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype\n",
      "---  ------      --------------  -----\n",
      " 0   categoryid  1644 non-null   int64\n",
      " 1   parentid    1644 non-null   int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 38.5 KB\n",
      "None\n",
      "   categoryid  parentid\n",
      "0        1016       213\n",
      "1         809       169\n",
      "2         570         9\n",
      "3        1691       885\n",
      "4         536      1691\n"
     ]
    }
   ],
   "source": [
    "# ---- Clean Category Tree ----\n",
    "cat = pd.read_csv(RAW_CAT)\n",
    "\n",
    "# Drop rows with missing ids\n",
    "cat = cat.dropna(subset=[\"categoryid\", \"parentid\"]).copy()\n",
    "\n",
    "# Cast to int \n",
    "cat[\"categoryid\"] = cat[\"categoryid\"].astype(int)\n",
    "cat[\"parentid\"]   = cat[\"parentid\"].astype(int)\n",
    "\n",
    "# Save\n",
    "cat.to_csv(OUT_CAT_CSV, index=False)\n",
    "\n",
    "# Preview\n",
    "print(\"== Cleaned Category Tree ==\")\n",
    "print(cat.info())\n",
    "print(cat.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b8f3b0",
   "metadata": {},
   "source": [
    "2. Cleaning the Events Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46ad6cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Cleaned User Events ==\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2755641 entries, 0 to 2755640\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Dtype         \n",
      "---  ------         -----         \n",
      " 0   timestamp      int64         \n",
      " 1   visitorid      int64         \n",
      " 2   event          object        \n",
      " 3   itemid         int64         \n",
      " 4   transactionid  float64       \n",
      " 5   event_time     datetime64[ns]\n",
      "dtypes: datetime64[ns](1), float64(1), int64(3), object(1)\n",
      "memory usage: 126.1+ MB\n",
      "None\n",
      "       timestamp  visitorid event  itemid  transactionid  \\\n",
      "0  1433221332117     257597  view  355908            NaN   \n",
      "1  1433224214164     992329  view  248676            NaN   \n",
      "2  1433221999827     111016  view  318965            NaN   \n",
      "3  1433221955914     483717  view  253185            NaN   \n",
      "4  1433221337106     951259  view  367447            NaN   \n",
      "\n",
      "               event_time  \n",
      "0 2015-06-02 05:02:12.117  \n",
      "1 2015-06-02 05:50:14.164  \n",
      "2 2015-06-02 05:13:19.827  \n",
      "3 2015-06-02 05:12:35.914  \n",
      "4 2015-06-02 05:02:17.106  \n"
     ]
    }
   ],
   "source": [
    "# ---- Clean User Events ----\n",
    "events = pd.read_csv(RAW_EVENTS)\n",
    "\n",
    "# Convert timestamp to datetime (ms â†’ ns)\n",
    "events[\"event_time\"] = pd.to_datetime(events[\"timestamp\"], unit=\"ms\", errors=\"coerce\")\n",
    "\n",
    "# Drop impossible rows (missing criticals)\n",
    "events = events.dropna(subset=[\"itemid\", \"event\", \"event_time\"]).copy()\n",
    "\n",
    "# Optional bot filter: users with > X events (tune or comment out)\n",
    "BOT_THRESHOLD = None  # e.g., 10_000; set None to skip\n",
    "if BOT_THRESHOLD is not None:\n",
    "    vc = events[\"visitorid\"].value_counts()\n",
    "    bots = vc[vc > BOT_THRESHOLD].index\n",
    "    events = events[~events[\"visitorid\"].isin(bots)].copy()\n",
    "\n",
    "# Drop duplicate full rows (safety)\n",
    "events = events.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Save\n",
    "events.to_csv(OUT_EVENTS_CSV, index=False)\n",
    "#events.to_parquet(OUT_EVENTS_PQ, index=False)\n",
    "\n",
    "# Preview\n",
    "print(\"== Cleaned User Events ==\")\n",
    "print(events.info())\n",
    "print(events.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3179b8",
   "metadata": {},
   "source": [
    "3. Cleaning the Item_properties datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ba5d731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Pass 1: compute latest timestamp per (itemid, property) across both files ==\n",
      "[item_properties_part1.1.csv] latest_ts_map: processed chunk 1\n",
      "[item_properties_part1.1.csv] latest_ts_map: processed chunk 2\n",
      "[item_properties_part1.1.csv] latest_ts_map: processed chunk 3\n",
      "[item_properties_part1.1.csv] latest_ts_map: processed chunk 4\n",
      "[item_properties_part1.1.csv] latest_ts_map: processed chunk 5\n",
      "[item_properties_part1.1.csv] latest_ts_map: processed chunk 6\n",
      "[item_properties_part1.1.csv] latest_ts_map: processed chunk 7\n",
      "[item_properties_part1.1.csv] latest_ts_map: processed chunk 8\n",
      "[item_properties_part1.1.csv] latest_ts_map: processed chunk 9\n",
      "[item_properties_part1.1.csv] latest_ts_map: processed chunk 10\n",
      "[item_properties_part1.1.csv] latest_ts_map: processed chunk 11\n",
      "[item_properties_part2.csv] latest_ts_map: processed chunk 1\n",
      "[item_properties_part2.csv] latest_ts_map: processed chunk 2\n",
      "[item_properties_part2.csv] latest_ts_map: processed chunk 3\n",
      "[item_properties_part2.csv] latest_ts_map: processed chunk 4\n",
      "[item_properties_part2.csv] latest_ts_map: processed chunk 5\n",
      "[item_properties_part2.csv] latest_ts_map: processed chunk 6\n",
      "[item_properties_part2.csv] latest_ts_map: processed chunk 7\n",
      "[item_properties_part2.csv] latest_ts_map: processed chunk 8\n",
      "[item_properties_part2.csv] latest_ts_map: processed chunk 9\n",
      "[item_properties_part2.csv] latest_ts_map: processed chunk 10\n",
      "Total unique (itemid, property) pairs: 12,003,814\n",
      "\n",
      "== Pass 2: collect rows with those latest timestamps and clean 'value' ==\n",
      "[item_properties_part1.1.csv] collect_latest_rows: matched rows in chunk 1: 592955\n",
      "[item_properties_part1.1.csv] collect_latest_rows: matched rows in chunk 2: 593869\n",
      "[item_properties_part1.1.csv] collect_latest_rows: matched rows in chunk 3: 593262\n",
      "[item_properties_part1.1.csv] collect_latest_rows: matched rows in chunk 4: 591818\n",
      "[item_properties_part1.1.csv] collect_latest_rows: matched rows in chunk 5: 590860\n",
      "[item_properties_part1.1.csv] collect_latest_rows: matched rows in chunk 6: 591053\n",
      "[item_properties_part1.1.csv] collect_latest_rows: matched rows in chunk 7: 591390\n",
      "[item_properties_part1.1.csv] collect_latest_rows: matched rows in chunk 8: 591765\n",
      "[item_properties_part1.1.csv] collect_latest_rows: matched rows in chunk 9: 591559\n",
      "[item_properties_part1.1.csv] collect_latest_rows: matched rows in chunk 10: 591732\n",
      "[item_properties_part1.1.csv] collect_latest_rows: matched rows in chunk 11: 590709\n",
      "[item_properties_part2.csv] collect_latest_rows: matched rows in chunk 1: 591374\n",
      "[item_properties_part2.csv] collect_latest_rows: matched rows in chunk 2: 592116\n",
      "[item_properties_part2.csv] collect_latest_rows: matched rows in chunk 3: 591391\n",
      "[item_properties_part2.csv] collect_latest_rows: matched rows in chunk 4: 590986\n",
      "[item_properties_part2.csv] collect_latest_rows: matched rows in chunk 5: 591618\n",
      "[item_properties_part2.csv] collect_latest_rows: matched rows in chunk 6: 593284\n",
      "[item_properties_part2.csv] collect_latest_rows: matched rows in chunk 7: 593589\n",
      "[item_properties_part2.csv] collect_latest_rows: matched rows in chunk 8: 593015\n",
      "[item_properties_part2.csv] collect_latest_rows: matched rows in chunk 9: 592123\n",
      "[item_properties_part2.csv] collect_latest_rows: matched rows in chunk 10: 163346\n",
      "\n",
      "== Preview: Cleaned Item Properties (long / latest only) ==\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12003814 entries, 0 to 12003813\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Dtype         \n",
      "---  ------        -----         \n",
      " 0   timestamp     int64         \n",
      " 1   itemid        int64         \n",
      " 2   property      object        \n",
      " 3   value         object        \n",
      " 4   timestamp_dt  datetime64[ns]\n",
      "dtypes: datetime64[ns](1), int64(2), object(2)\n",
      "memory usage: 457.9+ MB\n",
      "None\n",
      "       timestamp  itemid property      value        timestamp_dt\n",
      "0  1433646000000       0     1036  1276750.0 2015-06-07 03:00:00\n",
      "1  1433041200000       0     1056      3.168 2015-05-31 03:00:00\n",
      "2  1433646000000       0       11    15360.0 2015-06-07 03:00:00\n",
      "3  1433646000000       0      112   679677.0 2015-06-07 03:00:00\n",
      "4  1433041200000       0      127  1168476.0 2015-05-31 03:00:00\n",
      "Top properties by item coverage:\n",
      "property\n",
      "790           417053\n",
      "764           417053\n",
      "888           417053\n",
      "283           417053\n",
      "categoryid    417053\n",
      "364           417053\n",
      "available     417053\n",
      "112           417053\n",
      "159           417053\n",
      "678           417019\n",
      "Name: itemid, dtype: int64\n",
      " Parquet save failed: Error converting column \"value\" to bytes using encoding None. Original error: could not convert string to float: 'nInfinity'\n",
      " Fallback CSV saved: ..\\Data\\Cleaned Dataset\\cleaned_item_properties_latest.csv\n",
      "\n",
      "== Optional pivot to reduced wide table ==\n",
      "Selected 250 properties with coverage >= 5000 items.\n",
      "== Preview: Reduced Wide Item Features ==\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 417053 entries, 0 to 466866\n",
      "Columns: 250 entries, 0 to categoryid\n",
      "dtypes: float64(250)\n",
      "memory usage: 798.6 MB\n",
      "None\n",
      "property         0   1  101  1014  102  1028  1031  1032       1036  1037  \\\n",
      "itemid                                                                      \n",
      "0              NaN NaN  NaN   NaN  NaN   NaN   NaN   NaN  1276750.0   NaN   \n",
      "1         769062.0 NaN  NaN   NaN  NaN   NaN   NaN   NaN  1154859.0   NaN   \n",
      "2              NaN NaN  NaN   NaN  NaN   NaN   NaN   NaN        NaN   NaN   \n",
      "3              NaN NaN  NaN   NaN  NaN   NaN   NaN   NaN        NaN   NaN   \n",
      "4              NaN NaN  NaN   NaN  NaN   NaN   NaN   NaN        NaN   NaN   \n",
      "\n",
      "property  ...  976  978       981  982  987  988  993  994  available  \\\n",
      "itemid    ...                                                           \n",
      "0         ...  NaN  NaN       NaN  NaN  NaN  NaN  NaN  NaN        0.0   \n",
      "1         ...  NaN  NaN  769062.0  NaN  NaN  NaN  NaN  NaN        0.0   \n",
      "2         ...  NaN  NaN       NaN  NaN  NaN  NaN  NaN  NaN        0.0   \n",
      "3         ...  NaN  NaN       NaN  NaN  NaN  NaN  NaN  NaN        0.0   \n",
      "4         ...  NaN  NaN  819329.0  NaN  NaN  NaN  NaN  NaN        0.0   \n",
      "\n",
      "property  categoryid  \n",
      "itemid                \n",
      "0              209.0  \n",
      "1             1114.0  \n",
      "2             1305.0  \n",
      "3             1171.0  \n",
      "4             1038.0  \n",
      "\n",
      "[5 rows x 250 columns]\n",
      " Saved Parquet: ..\\Data\\Cleaned Dataset\\cleaned_item_properties_wide.parquet\n",
      "\n",
      "âœ… Item properties cleaning complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "OUT_IP_LATEST_PARQUET = CLEAN_DIR / \"cleaned_item_properties_latest.parquet\"\n",
    "OUT_IP_LATEST_CSV     = CLEAN_DIR / \"cleaned_item_properties_latest.csv\"\n",
    "\n",
    "# Optional reduced wide (pivot) outputs\n",
    "OUT_IP_WIDE_PARQUET = CLEAN_DIR / \"cleaned_item_properties_wide.parquet\"\n",
    "OUT_IP_WIDE_CSV     = CLEAN_DIR / \"cleaned_item_properties_wide.csv\"\n",
    "\n",
    "# ---- Config ----\n",
    "CHUNKSIZE = 1_000_000  # tune down if memory is tight (e.g., 500_000 or 250_000)\n",
    "PRINT_EVERY = 1        # progress print frequency (chunks)\n",
    "\n",
    "# Reduced-wide pivot selection (optional step)\n",
    "MIN_ITEMS_FOR_PROPERTY = 5_000   # keep properties appearing on >= this many items\n",
    "TOP_K_PROPERTIES = None          # or set e.g., 200 (takes precedence if not None)\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "\n",
    "# Safe saver: prefer fastparquet; fallback to CSV (never crash)\n",
    "def save_dataframe(df: pd.DataFrame, parquet_path: Path, csv_path: Path):\n",
    "    parquet_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    try:\n",
    "        import fastparquet  # noqa: F401\n",
    "        df.to_parquet(parquet_path, index=False, engine=\"fastparquet\")\n",
    "        print(f\" Saved Parquet: {parquet_path}\")\n",
    "    except ImportError:\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\" fastparquet not installed. Saved CSV instead: {csv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\" Parquet save failed: {e}\")\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\" Fallback CSV saved: {csv_path}\")\n",
    "\n",
    "# Value normalizer:\n",
    "# - keeps booleans (true/false -> 1/0)\n",
    "# - strips leading 'n' then extracts first number (e.g., \"n720.000 424566\" -> 720.0)\n",
    "# - returns original string if non-numeric\n",
    "_num_pat = re.compile(r\"[-+]?\\d*\\.?\\d+\")\n",
    "def clean_value(v):\n",
    "    if pd.isna(v):\n",
    "        return np.nan\n",
    "    if isinstance(v, (int, float, np.integer, np.floating)):\n",
    "        return float(v)\n",
    "    s = str(v).strip().lower()\n",
    "    if s in {\"true\", \"yes\"}:\n",
    "        return 1.0\n",
    "    if s in {\"false\", \"no\"}:\n",
    "        return 0.0\n",
    "    # strip a single leading 'n' (common pattern) before extracting number\n",
    "    if s.startswith(\"n\"):\n",
    "        s = s[1:]\n",
    "    m = _num_pat.search(s)\n",
    "    return float(m.group()) if m else v  # keep original string if not numeric\n",
    "\n",
    "def latest_ts_map(csv_path: Path, chunksize=CHUNKSIZE):\n",
    "    \"\"\"\n",
    "    First pass: build a dict mapping (itemid, property) -> latest timestamp.\n",
    "    Uses groupby per chunk for efficiency.\n",
    "    \"\"\"\n",
    "    latest = {}  # key: (itemid, property) -> int timestamp\n",
    "    usecols = [\"timestamp\", \"itemid\", \"property\", \"value\"]\n",
    "    for i, chunk in enumerate(pd.read_csv(csv_path, usecols=usecols, chunksize=chunksize, low_memory=False)):\n",
    "        # keep only necessary rows\n",
    "        chunk = chunk.dropna(subset=[\"itemid\", \"property\", \"timestamp\"])\n",
    "        # type enforcement\n",
    "        # (avoid astype on entire columns if it triggers memory; rely on numpy casting while grouping)\n",
    "        grp = chunk.groupby([\"itemid\", \"property\"], as_index=False)[\"timestamp\"].max()\n",
    "        # update dict\n",
    "        for r in grp.itertuples(index=False):\n",
    "            key = (int(r.itemid), str(r.property))\n",
    "            ts  = int(r.timestamp)\n",
    "            if key not in latest or ts > latest[key]:\n",
    "                latest[key] = ts\n",
    "        if (i + 1) % PRINT_EVERY == 0:\n",
    "            print(f\"[{csv_path.name}] latest_ts_map: processed chunk {i+1}\")\n",
    "    return latest\n",
    "\n",
    "def collect_latest_rows(csv_path: Path, latest_map: dict, chunksize=CHUNKSIZE):\n",
    "    \"\"\"\n",
    "    Second pass: for rows that match the latest timestamp per (item, property),\n",
    "    collect them, clean 'value', and return a concatenated dataframe (long format).\n",
    "    \"\"\"\n",
    "    usecols = [\"timestamp\", \"itemid\", \"property\", \"value\"]\n",
    "    out_parts = []\n",
    "    for i, chunk in enumerate(pd.read_csv(csv_path, usecols=usecols, chunksize=chunksize, low_memory=False)):\n",
    "        chunk = chunk.dropna(subset=[\"itemid\", \"property\", \"timestamp\"]).copy()\n",
    "        # normalize types for comparison\n",
    "        chunk[\"itemid\"]    = chunk[\"itemid\"].astype(np.int64, copy=False)\n",
    "        chunk[\"property\"]  = chunk[\"property\"].astype(str)\n",
    "        chunk[\"timestamp\"] = chunk[\"timestamp\"].astype(np.int64, copy=False)\n",
    "\n",
    "        # Build boolean mask for rows that are the latest per (item, property)\n",
    "        # Using vectorized approach via DataFrame apply would be slow; do a python-level check\n",
    "        idx = []\n",
    "        it_item = chunk[\"itemid\"].values\n",
    "        it_prop = chunk[\"property\"].values\n",
    "        it_ts   = chunk[\"timestamp\"].values\n",
    "        for j in range(len(chunk)):\n",
    "            key = (int(it_item[j]), it_prop[j])\n",
    "            if key in latest_map and latest_map[key] == int(it_ts[j]):\n",
    "                idx.append(j)\n",
    "\n",
    "        if idx:\n",
    "            matched = chunk.iloc[idx, :].copy()\n",
    "            # clean value\n",
    "            matched.loc[:, \"value\"] = matched[\"value\"].apply(clean_value)\n",
    "            out_parts.append(matched)\n",
    "\n",
    "        if (i + 1) % PRINT_EVERY == 0:\n",
    "            print(f\"[{csv_path.name}] collect_latest_rows: matched rows in chunk {i+1}: {len(idx)}\")\n",
    "\n",
    "    if out_parts:\n",
    "        res = pd.concat(out_parts, ignore_index=True)\n",
    "    else:\n",
    "        res = pd.DataFrame(columns=[\"itemid\", \"property\", \"timestamp\", \"value\"])\n",
    "    return res\n",
    "\n",
    "# =========================\n",
    "# Run Cleaning (two passes)\n",
    "# =========================\n",
    "\n",
    "print(\"== Pass 1: compute latest timestamp per (itemid, property) across both files ==\")\n",
    "latest1 = latest_ts_map(RAW_IP1)\n",
    "latest2 = latest_ts_map(RAW_IP2)\n",
    "\n",
    "# merge the two dicts, keeping max ts per key\n",
    "latest = latest1\n",
    "for k, ts in latest2.items():\n",
    "    if k not in latest or ts > latest[k]:\n",
    "        latest[k] = ts\n",
    "\n",
    "print(f\"Total unique (itemid, property) pairs: {len(latest):,}\")\n",
    "\n",
    "print(\"\\n== Pass 2: collect rows with those latest timestamps and clean 'value' ==\")\n",
    "latest_rows_1 = collect_latest_rows(RAW_IP1, latest)\n",
    "latest_rows_2 = collect_latest_rows(RAW_IP2, latest)\n",
    "\n",
    "latest_rows = pd.concat([latest_rows_1, latest_rows_2], ignore_index=True)\n",
    "\n",
    "# Deduplicate in case both files had the same (item, property, timestamp)\n",
    "latest_rows.sort_values([\"itemid\", \"property\", \"timestamp\"], ascending=[True, True, False], inplace=True)\n",
    "latest_rows = latest_rows.drop_duplicates(subset=[\"itemid\", \"property\"], keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "# (Optional) add a datetime column for readability (kept numeric ts for joins/perf)\n",
    "latest_rows[\"timestamp_dt\"] = pd.to_datetime(latest_rows[\"timestamp\"], unit=\"ms\", errors=\"coerce\")\n",
    "\n",
    "print(\"\\n== Preview: Cleaned Item Properties (long / latest only) ==\")\n",
    "print(latest_rows.info())\n",
    "print(latest_rows.head())\n",
    "print(\"Top properties by item coverage:\")\n",
    "print(latest_rows.groupby(\"property\")[\"itemid\"].nunique().sort_values(ascending=False).head(10))\n",
    "\n",
    "# Save (safe)\n",
    "save_dataframe(latest_rows, OUT_IP_LATEST_PARQUET, OUT_IP_LATEST_CSV)\n",
    "\n",
    "# =========================\n",
    "# (Optional) Reduced Wide Pivot\n",
    "# =========================\n",
    "DO_PIVOT = True  # set False if you want to skip\n",
    "\n",
    "if DO_PIVOT:\n",
    "    print(\"\\n== Optional pivot to reduced wide table ==\")\n",
    "    prop_cov = latest_rows.groupby(\"property\")[\"itemid\"].nunique().sort_values(ascending=False)\n",
    "\n",
    "    if TOP_K_PROPERTIES:\n",
    "        props_keep = prop_cov.head(TOP_K_PROPERTIES).index.tolist()\n",
    "        print(f\"Selected top {len(props_keep)} properties by coverage.\")\n",
    "    else:\n",
    "        props_keep = prop_cov[prop_cov >= MIN_ITEMS_FOR_PROPERTY].index.tolist()\n",
    "        print(f\"Selected {len(props_keep)} properties with coverage >= {MIN_ITEMS_FOR_PROPERTY} items.\")\n",
    "\n",
    "    reduced = latest_rows[latest_rows[\"property\"].isin(props_keep)].copy()\n",
    "\n",
    "    # Pivot to wide: columns = property, index = itemid\n",
    "    wide = reduced.pivot(index=\"itemid\", columns=\"property\", values=\"value\")\n",
    "\n",
    "    # Best-effort numeric conversion: keep numeric if it makes sense\n",
    "    for c in wide.columns:\n",
    "        if wide[c].dtype == object:\n",
    "            converted = pd.to_numeric(wide[c], errors=\"coerce\")\n",
    "            # keep numeric if at least half of non-nulls converted\n",
    "            if converted.notna().sum() >= 0.5 * wide[c].notna().sum():\n",
    "                wide[c] = converted\n",
    "\n",
    "    print(\"== Preview: Reduced Wide Item Features ==\")\n",
    "    print(wide.info())\n",
    "    print(wide.head())\n",
    "\n",
    "    # Save (safe) â€” reset_index to keep itemid as a column in CSV\n",
    "    save_dataframe(wide.reset_index(), OUT_IP_WIDE_PARQUET, OUT_IP_WIDE_CSV)\n",
    "\n",
    "print(\"\\n     Item properties cleaning complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfef06d0",
   "metadata": {},
   "source": [
    "4. Merge all 3 dataset into 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f36b446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Building item features (categoryid, available) from long properties...\n",
      "  - Processed chunk 1: 34,706 item rows\n",
      "  - Processed chunk 2: 34,760 item rows\n",
      "  - Processed chunk 3: 34,741 item rows\n",
      "  - Processed chunk 4: 34,748 item rows\n",
      "  - Processed chunk 5: 34,767 item rows\n",
      "  - Processed chunk 6: 34,779 item rows\n",
      "  - Processed chunk 7: 34,758 item rows\n",
      "  - Processed chunk 8: 34,770 item rows\n",
      "  - Processed chunk 9: 34,738 item rows\n",
      "  - Processed chunk 10: 34,724 item rows\n",
      "  - Processed chunk 11: 34,724 item rows\n",
      "  - Processed chunk 12: 34,710 item rows\n",
      "  - Processed chunk 13: 128 item rows\n",
      "\n",
      "Item features built:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 417053 entries, 0 to 417052\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype\n",
      "---  ------      --------------   -----\n",
      " 0   itemid      417053 non-null  int64\n",
      " 1   available   417053 non-null  Int8 \n",
      " 2   categoryid  417053 non-null  Int64\n",
      "dtypes: Int64(1), Int8(1), int64(1)\n",
      "memory usage: 7.6 MB\n",
      "None\n",
      "property  itemid  available  categoryid\n",
      "0              0          0         209\n",
      "1              1          0        1114\n",
      "2              2          0        1305\n",
      "3              3          0        1171\n",
      "4              4          0        1038\n",
      " Saved Parquet: ..\\Data\\Cleaned Dataset\\item_features.parquet\n",
      "\n",
      "Step 2: Joining category tree onto item features...\n",
      "Item features + category tree:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 417053 entries, 0 to 417052\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count   Dtype\n",
      "---  ------      --------------   -----\n",
      " 0   itemid      417053 non-null  int64\n",
      " 1   available   417053 non-null  Int8 \n",
      " 2   categoryid  417053 non-null  Int64\n",
      " 3   parentid    416896 non-null  Int64\n",
      "dtypes: Int64(2), Int8(1), int64(1)\n",
      "memory usage: 11.1 MB\n",
      "None\n",
      "   itemid  available  categoryid  parentid\n",
      "0       0          0         209       293\n",
      "1       1          0        1114       113\n",
      "2       2          0        1305      1214\n",
      "3       3          0        1171       938\n",
      "4       4          0        1038      1174\n",
      " Saved Parquet: ..\\Data\\Cleaned Dataset\\item_features.parquet\n",
      "\n",
      "Step 3: Chunk-merging events with item features...\n",
      "  - Merged event chunk 1: 500,000 rows (cumulative 500,000)\n",
      "  - Merged event chunk 2: 500,000 rows (cumulative 1,000,000)\n",
      "  - Merged event chunk 3: 500,000 rows (cumulative 1,500,000)\n",
      "  - Merged event chunk 4: 500,000 rows (cumulative 2,000,000)\n",
      "  - Merged event chunk 5: 500,000 rows (cumulative 2,500,000)\n",
      "  - Merged event chunk 6: 255,641 rows (cumulative 2,755,641)\n",
      "\n",
      " Final merged (CSV) written to: ..\\Data\\Cleaned Dataset\\final_merged_events.csv\n",
      "Tip: Convert large CSV to Parquet later if needed (e.g., using pandas with fastparquet).\n",
      "\n",
      " Merge pipeline complete.\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Final Merge Pipeline (Slim)\n",
    "# ============================\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# -----------------\n",
    "# Paths & settings\n",
    "# -----------------\n",
    "DATA_DIR   = Path(\"../Data/Cleaned Dataset\")\n",
    "OUT_DIR    = DATA_DIR  # keep outputs alongside cleaned data\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "EVENTS_CSV      = DATA_DIR / \"cleaned_user_events.csv\"\n",
    "CAT_TREE_CSV    = DATA_DIR / \"cleaned_category_tree.csv\"\n",
    "IP_LATEST_CSV   = DATA_DIR / \"cleaned_item_properties_latest.csv\"   # long format (12M rows)\n",
    "\n",
    "# Keep the item properties we really need for the first pass\n",
    "\n",
    "PROPS_TO_KEEP = {\"categoryid\", \"available\"}\n",
    "\n",
    "# Output files\n",
    "ITEM_FEATURES_PARQ  = OUT_DIR / \"item_features.parquet\"\n",
    "ITEM_FEATURES_CSV   = OUT_DIR / \"item_features.csv\"\n",
    "FINAL_MERGED_PARQ   = OUT_DIR / \"final_merged_events.parquet\"\n",
    "FINAL_MERGED_CSV    = OUT_DIR / \"final_merged_events.csv\"\n",
    "\n",
    "# Chunk sizes\n",
    "IP_CHUNKSIZE     = 1_000_000   # reading item_properties_latest.csv\n",
    "EVENTS_CHUNKSIZE = 500_000     # chunk-merge events with item features\n",
    "\n",
    "# -----------------\n",
    "# Helpers\n",
    "# -----------------\n",
    "def safe_save(df, parquet_path: Path, csv_path: Path):\n",
    "    \"\"\"Try Parquet with fastparquet, else fallback to CSV (never crash).\"\"\"\n",
    "    parquet_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    try:\n",
    "        import fastparquet  # noqa: F401\n",
    "        df.to_parquet(parquet_path, index=False, engine=\"fastparquet\")\n",
    "        print(f\" Saved Parquet: {parquet_path}\")\n",
    "    except ImportError:\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\" fastparquet not installed. Saved CSV instead: {csv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\" Parquet save failed: {e}\")\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\" Fallback CSV saved: {csv_path}\")\n",
    "\n",
    "def coerce_int(series):\n",
    "    \"\"\"Coerce to pandas nullable Int64 safely.\"\"\"\n",
    "    return pd.to_numeric(series, errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "def coerce_binary(series):\n",
    "    \"\"\"Coerce 'available' (mixed) to 0/1 Int8 where possible.\"\"\"\n",
    "    s = pd.to_numeric(series, errors=\"coerce\")\n",
    "    # clamp to {0,1} if values are near binary\n",
    "    s = s.fillna(0)\n",
    "    s = (s > 0).astype(\"Int8\")\n",
    "    return s\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 1) Build a slim item-features table from long props\n",
    "# ---------------------------------------------------\n",
    "print(\"Step 1: Building item features (categoryid, available) from long properties...\")\n",
    "item_feat_parts = []\n",
    "\n",
    "usecols = [\"itemid\", \"property\", \"value\"]  # timestamp is already 'latest' in this file\n",
    "reader = pd.read_csv(\n",
    "    IP_LATEST_CSV,\n",
    "    usecols=usecols,\n",
    "    dtype={\"value\": \"string\"},   # avoid DtypeWarning for mixed types\n",
    "    chunksize=IP_CHUNKSIZE,\n",
    "    low_memory=False\n",
    ")\n",
    "\n",
    "for i, chunk in enumerate(reader, start=1):\n",
    "    # Keep only the properties we care about\n",
    "    chunk = chunk[chunk[\"property\"].isin(PROPS_TO_KEEP)].copy()\n",
    "    if chunk.empty:\n",
    "        print(f\"  - Chunk {i}: no target properties, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Long -> Wide per chunk\n",
    "    # Because this is \"latest\" file, each (itemid, property) should be unique.\n",
    "    wide = chunk.pivot_table(\n",
    "        index=\"itemid\",\n",
    "        columns=\"property\",\n",
    "        values=\"value\",\n",
    "        aggfunc=\"last\"  # safeguard\n",
    "    )\n",
    "\n",
    "    # Type cleanup\n",
    "    if \"categoryid\" in wide.columns:\n",
    "        wide[\"categoryid\"] = coerce_int(wide[\"categoryid\"])\n",
    "    if \"available\" in wide.columns:\n",
    "        wide[\"available\"] = coerce_binary(wide[\"available\"])\n",
    "\n",
    "    # Reset index so itemid becomes a column\n",
    "    wide = wide.reset_index()\n",
    "    item_feat_parts.append(wide)\n",
    "\n",
    "    print(f\"  - Processed chunk {i}: {len(wide):,} item rows\")\n",
    "\n",
    "# Concatenate all parts and collapse duplicates (last wins)\n",
    "if item_feat_parts:\n",
    "    item_features = pd.concat(item_feat_parts, ignore_index=True)\n",
    "    item_features.sort_values(\"itemid\", inplace=True)\n",
    "    item_features = item_features.drop_duplicates(subset=\"itemid\", keep=\"last\").reset_index(drop=True)\n",
    "else:\n",
    "    # Edge case: nothing found\n",
    "    item_features = pd.DataFrame({\"itemid\": pd.Series(dtype=\"Int64\")})\n",
    "    for p in PROPS_TO_KEEP:\n",
    "        item_features[p] = pd.Series(dtype=\"float\")\n",
    "\n",
    "print(\"\\nItem features built:\")\n",
    "print(item_features.info())\n",
    "print(item_features.head())\n",
    "\n",
    "# Save slim item features now (so we can reuse later quickly)\n",
    "safe_save(item_features, ITEM_FEATURES_PARQ, ITEM_FEATURES_CSV)\n",
    "\n",
    "# ------------------------------------------\n",
    "# 2) Join category hierarchy onto features\n",
    "# ------------------------------------------\n",
    "print(\"\\nStep 2: Joining category tree onto item features...\")\n",
    "cat_tree = pd.read_csv(CAT_TREE_CSV)\n",
    "# ensure ints\n",
    "cat_tree[\"categoryid\"] = coerce_int(cat_tree[\"categoryid\"])\n",
    "cat_tree[\"parentid\"]   = coerce_int(cat_tree[\"parentid\"])\n",
    "\n",
    "item_features = item_features.merge(cat_tree, on=\"categoryid\", how=\"left\")\n",
    "\n",
    "print(\"Item features + category tree:\")\n",
    "print(item_features.info())\n",
    "print(item_features.head())\n",
    "\n",
    "# (Optionally) re-save enriched item features\n",
    "safe_save(item_features, ITEM_FEATURES_PARQ, ITEM_FEATURES_CSV)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 3) Chunk-merge Events with item features (low memory)\n",
    "# -------------------------------------------------------\n",
    "print(\"\\nStep 3: Chunk-merging events with item features...\")\n",
    "\n",
    "# prepare output file (CSV fall-back path)\n",
    "final_out_csv  = FINAL_MERGED_CSV\n",
    "final_out_parq = FINAL_MERGED_PARQ\n",
    "\n",
    "# If a previous CSV exists, remove to avoid appending to old file\n",
    "if final_out_csv.exists():\n",
    "    final_out_csv.unlink()\n",
    "\n",
    "# Weâ€™ll write CSV incrementally to be safe across environments.\n",
    "reader_events = pd.read_csv(\n",
    "    EVENTS_CSV,\n",
    "    parse_dates=[\"event_time\"],\n",
    "    chunksize=EVENTS_CHUNKSIZE,\n",
    "    low_memory=False\n",
    ")\n",
    "\n",
    "first = True\n",
    "total_rows = 0\n",
    "for j, echunk in enumerate(reader_events, start=1):\n",
    "    merged = echunk.merge(item_features, on=\"itemid\", how=\"left\")\n",
    "    total_rows += len(merged)\n",
    "\n",
    "    # incremental CSV write\n",
    "    merged.to_csv(final_out_csv, mode=\"a\", index=False, header=first)\n",
    "    first = False\n",
    "\n",
    "    print(f\"  - Merged event chunk {j}: {len(merged):,} rows (cumulative {total_rows:,})\")\n",
    "\n",
    "print(f\"\\n Final merged (CSV) written to: {final_out_csv}\")\n",
    "\n",
    "# Try to also write a Parquet (best-effort)\n",
    "try:\n",
    "    import fastparquet  # noqa: F401\n",
    "    # If file is large, re-read the CSV in chunks and write a single parquet is non-trivial.\n",
    "    # As a simple path, leave parquet out or convert later with an external tool.\n",
    "    print(\"Tip: Convert large CSV to Parquet later if needed (e.g., using pandas with fastparquet).\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print(\"\\n Merge pipeline complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495dd7e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

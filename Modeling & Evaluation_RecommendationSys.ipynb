{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6366bbd",
   "metadata": {},
   "source": [
    "## MODELLING AND EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81f5a15",
   "metadata": {},
   "source": [
    " Goal: To build a practical **offline recommender** that generates high-quality top-K suggestions using a multi-method candidate pool and simple business-aware re-ranking. We evaluate with standard ranking metrics on a **time-ordered split** to mimic production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "842d12ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.sparse import coo_matrix, csr_matrix, diags\n",
    "from scipy.sparse.linalg import svds\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379754ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Imports & Config\n",
    "\n",
    "# --- global config ---\n",
    "K = 10                         # top-K for evaluation\n",
    "EVENT_WEIGHTS = {'view': 1.0, 'addtocart': 3.0, 'transaction': 5.0}\n",
    "MIN_TRAIN_INTERACTIONS = 2     # drop ultra-sparse train users\n",
    "CHUNK_USERS = 25000            # evaluation in chunks to avoid long single loops\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"../Data/Cleaned Dataset/final_merged_events.csv\")\n",
    "\n",
    "# Ensure timestamp as datetime (use event_time if you prefer)\n",
    "if 'event_time' in df.columns:\n",
    "    df['event_time'] = pd.to_datetime(df['event_time'], errors='coerce')\n",
    "    df['ts_dt'] = df['event_time']\n",
    "else:\n",
    "    df['ts_dt'] = pd.to_datetime(df['timestamp'], unit='ms', errors='coerce')\n",
    "\n",
    "# Keep only needed cols\n",
    "df = df[['visitorid','itemid','event','ts_dt']].dropna(subset=['visitorid','itemid','event','ts_dt']).copy()\n",
    "\n",
    "# =========================\n",
    "# 2) Weighting & Train/Test Split (chronological leave-last-out)\n",
    "# =========================\n",
    "df['w'] = df['event'].map(EVENT_WEIGHTS).fillna(0.5)  # unseen events get small weight\n",
    "\n",
    "# sort by time per user\n",
    "df = df.sort_values(['visitorid','ts_dt'])\n",
    "\n",
    "# last interaction per user -> test; others -> train\n",
    "last_idx = df.groupby('visitorid')['ts_dt'].idxmax()\n",
    "df['split'] = 'train'\n",
    "df.loc[last_idx, 'split'] = 'test'\n",
    "\n",
    "train = df[df['split']=='train'].copy()\n",
    "test  = df[df['split']=='test'].copy()\n",
    "\n",
    "# drop users with too few train interactions (optional, improves CF stability)\n",
    "train_counts = train['visitorid'].value_counts()\n",
    "keep_users = set(train_counts[train_counts >= MIN_TRAIN_INTERACTIONS].index)\n",
    "train = train[train['visitorid'].isin(keep_users)]\n",
    "test  = test[test['visitorid'].isin(keep_users)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1cb229d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train matrix: (199983, 125261) | nnz: 722658\n",
      "Eval users (with test truth & in-train): 190636\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =========================\n",
    "# 3) Build ID Mappings & Sparse User–Item Matrix (train)\n",
    "# =========================\n",
    "unique_users = train['visitorid'].unique()\n",
    "unique_items = train['itemid'].unique()\n",
    "\n",
    "user2idx = {u:i for i,u in enumerate(unique_users)}\n",
    "item2idx = {i:j for j,i in enumerate(unique_items)}\n",
    "idx2user = np.array(unique_users)\n",
    "idx2item = np.array(unique_items)\n",
    "\n",
    "# remap train\n",
    "tr_u = train['visitorid'].map(user2idx).values\n",
    "tr_i = train['itemid'].map(item2idx).values\n",
    "tr_w = train['w'].values\n",
    "\n",
    "# sum duplicates by coo -> csr\n",
    "X_train = coo_matrix((tr_w, (tr_u, tr_i)), shape=(len(unique_users), len(unique_items))).tocsr()\n",
    "\n",
    "n_users, n_items = X_train.shape\n",
    "print(\"Train matrix:\", X_train.shape, \"| nnz:\", X_train.nnz)\n",
    "\n",
    "# seen items per user (train)\n",
    "seen_train = {}\n",
    "X_train_csr = X_train.tocsr()\n",
    "for u in range(n_users):\n",
    "    start, end = X_train_csr.indptr[u], X_train_csr.indptr[u+1]\n",
    "    seen_train[u] = set(X_train_csr.indices[start:end])\n",
    "\n",
    "# test ground truth (only items that exist in train space)\n",
    "test_truth = defaultdict(set)\n",
    "test_mapped = test[test['itemid'].isin(item2idx)].copy()\n",
    "for r in test_mapped.itertuples(index=False):\n",
    "    u = r.visitorid\n",
    "    i = r.itemid\n",
    "    test_truth[u].add(i)\n",
    "\n",
    "# restrict evaluation users to those present in train mapping and with >0 ground truth\n",
    "eval_users = [u for u in test_truth.keys() if u in user2idx]\n",
    "print(\"Eval users (with test truth & in-train):\", len(eval_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3c0c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4) Utility: Metrics\n",
    "\n",
    "def recall_at_k(true_items, rec_list, k=10):\n",
    "    if not true_items: return 0.0\n",
    "    return len(set(rec_list[:k]) & set(true_items)) / len(true_items)\n",
    "\n",
    "def ap_at_k(true_items, rec_list, k=10):\n",
    "    if not true_items: return 0.0\n",
    "    true = set(true_items)\n",
    "    hits, ap = 0, 0.0\n",
    "    for i, it in enumerate(rec_list[:k], start=1):\n",
    "        if it in true:\n",
    "            hits += 1\n",
    "            ap += hits / i\n",
    "    return ap / min(len(true_items), k)\n",
    "\n",
    "def ndcg_at_k(true_items, rec_list, k=10):\n",
    "    if not true_items: return 0.0\n",
    "    true = set(true_items)\n",
    "    dcg = 0.0\n",
    "    for i, it in enumerate(rec_list[:k], start=1):\n",
    "        if it in true:\n",
    "            dcg += 1.0 / np.log2(i+1)\n",
    "    ideal = min(len(true_items), k)\n",
    "    idcg = sum(1.0 / np.log2(i+1) for i in range(1, ideal+1))\n",
    "    return dcg / idcg if idcg > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f39dd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 5) Popularity Baseline\n",
    "# =========================\n",
    "# global top items by train weight (heavier weight => more important)\n",
    "item_pop = np.asarray(X_train.sum(axis=0)).ravel()\n",
    "pop_order = np.argsort(-item_pop)  # item indices (train-space)\n",
    "\n",
    "def toplist_pop(u_idx, k=K):\n",
    "    # same list for all users (filter seen to avoid duplicates)\n",
    "    seen = seen_train.get(u_idx, set())\n",
    "    out = []\n",
    "    for j in pop_order:\n",
    "        if j not in seen:\n",
    "            out.append(j)\n",
    "            if len(out) == k: break\n",
    "    return [idx2item[j] for j in out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfe66203",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# 6) Weightings: TF-IDF & BM25\n",
    "# =========================\n",
    "def tfidf_weight(X):\n",
    "    \"\"\" Right-multiply by IDF on items (columns). \"\"\"\n",
    "    # df_i: number of users who interacted with item i\n",
    "    df_i = np.diff(X.tocsc().indptr)  # fast column nnz\n",
    "    idf = np.log((X.shape[0] + 1) / (df_i + 1)) + 1.0\n",
    "    W = diags(idf)\n",
    "    return (X @ W).tocsr()\n",
    "\n",
    "def bm25_weight(X, K1=100, B=0.8):\n",
    "    \"\"\"\n",
    "    BM25 for implicit feedback treating each user-row as a document.\n",
    "    \"\"\"\n",
    "    X = X.tocsr().astype(np.float64)\n",
    "    # document lengths per user\n",
    "    row_sums = np.asarray(X.sum(axis=1)).ravel()\n",
    "    avg_len = row_sums.mean() + 1e-9\n",
    "\n",
    "    # df per item\n",
    "    df_i = np.diff(X.tocsc().indptr).astype(np.float64)\n",
    "    idf = np.log((X.shape[0] - df_i + 0.5) / (df_i + 0.5))\n",
    "    idf = np.maximum(idf, 0)  # clamp negatives\n",
    "\n",
    "    X_bm25 = X.copy().tocoo()\n",
    "    # term frequency (here: weights) with BM25 saturation\n",
    "    norm = K1 * (1 - B + B * (row_sums[X_bm25.row] / avg_len))\n",
    "    X_bm25.data = (X_bm25.data * (K1 + 1)) / (X_bm25.data + norm)\n",
    "    # apply idf\n",
    "    X_bm25.data *= idf[X_bm25.col]\n",
    "    return X_bm25.tocsr()\n",
    "\n",
    "# Precompute variants\n",
    "X_RAW   = X_train\n",
    "X_TFIDF = tfidf_weight(X_train)\n",
    "X_BM25  = bm25_weight(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1f9f34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 7) Item–Item Cosine (memory-safe, no full similarity)\n",
    "#    scores = (w * C^T) * C, where C = column-L2-normalized X\n",
    "# =========================\n",
    "def prepare_itemcosine_inputs(X):\n",
    "    # column norms\n",
    "    col_sq = np.ravel(X.power(2).sum(axis=0))\n",
    "    col_norm = np.sqrt(col_sq) + 1e-12\n",
    "    inv = 1.0 / col_norm\n",
    "    C = (X @ diags(inv)).tocsr()  # users x items, col-normalized\n",
    "    return C, inv  # inv kept just in case\n",
    "\n",
    "def recommend_itemcosine(u_idx, X, C, k=K):\n",
    "    \"\"\"Recommend via item–item cosine using two-step multiplication (sparse-safe).\"\"\"\n",
    "    seen = seen_train.get(u_idx, set())\n",
    "    # w = user row over items\n",
    "    w = X[u_idx, :]                      # 1 x n_items\n",
    "    # y = w * C^T  -> shape (1, n_users)\n",
    "    y = (w @ C.T)                        # sparse * sparse -> sparse\n",
    "    # scores = y * C -> shape (1, n_items)\n",
    "    s = (y @ C).toarray().ravel()        # dense 1D\n",
    "    if seen:\n",
    "        s[list(seen)] = -np.inf\n",
    "    # top-k\n",
    "    top = np.argpartition(-s, k)[:k]\n",
    "    top = top[np.argsort(-s[top])]\n",
    "    return [idx2item[j] for j in top]\n",
    "\n",
    "# prepare cosine inputs for each weighting\n",
    "C_RAW,   _ = prepare_itemcosine_inputs(X_RAW)\n",
    "C_TFIDF, _ = prepare_itemcosine_inputs(X_TFIDF)\n",
    "C_BM25,  _ = prepare_itemcosine_inputs(X_BM25)\n",
    "\n",
    "# wrappers\n",
    "def toplist_itemcosine_raw(u_idx, k=K):   return recommend_itemcosine(u_idx, X_RAW,   C_RAW,   k)\n",
    "def toplist_itemcosine_tfidf(u_idx, k=K): return recommend_itemcosine(u_idx, X_TFIDF, C_TFIDF, k)\n",
    "def toplist_itemcosine_bm25(u_idx, k=K):  return recommend_itemcosine(u_idx, X_BM25,  C_BM25,  k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b3855c2",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# 8) SVD Latent Factors (pure SciPy)\n",
    "#    X ≈ U S V^T; use P = U sqrt(S), Q = V sqrt(S) for scoring: score = P[u]·Q^T\n",
    "\n",
    "def train_svd(X, factors=64, random_state=RANDOM_STATE):\n",
    "    U, s, Vt = svds(X.asfptype(), k=factors, which='LM', return_singular_vectors=True)\n",
    "    # sort largest -> smallest\n",
    "    order = np.argsort(-s)\n",
    "    s = s[order]\n",
    "    U = U[:, order]\n",
    "    Vt = Vt[order, :]\n",
    "    Ssqrt = np.sqrt(s)\n",
    "    P = U * Ssqrt   # users x k\n",
    "    Q = (Vt.T * Ssqrt)  # items x k\n",
    "    # L2 normalize rows (optional, often helps)\n",
    "    Pu = np.linalg.norm(P, axis=1, keepdims=True) + 1e-12\n",
    "    Qi = np.linalg.norm(Q, axis=1, keepdims=True) + 1e-12\n",
    "    return (P / Pu), (Q / Qi)\n",
    "\n",
    "def recommend_svd(u_idx, P, Q, k=K):\n",
    "    seen = seen_train.get(u_idx, set())\n",
    "    scores = (P[u_idx, :] @ Q.T)   # 1 x n_items\n",
    "    if seen:\n",
    "        scores[list(seen)] = -np.inf\n",
    "    top = np.argpartition(-scores, k)[:k]\n",
    "    top = top[np.argsort(-scores[top])]\n",
    "    return [idx2item[j] for j in top]\n",
    "\n",
    "# Train SVD on the strongest weighting (BM25 usually best)\n",
    "P_bm25, Q_bm25 = train_svd(X_BM25, factors=64)\n",
    "def toplist_svd_bm25(u_idx, k=K): return recommend_svd(u_idx, P_bm25, Q_bm25, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c6da26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Popularity@10] processed: 2000/190636\n",
      "[Popularity@10] processed: 4000/190636\n",
      "[Popularity@10] processed: 6000/190636\n",
      "[Popularity@10] processed: 8000/190636\n",
      "[Popularity@10] processed: 10000/190636\n",
      "[Popularity@10] processed: 12000/190636\n",
      "[Popularity@10] processed: 14000/190636\n",
      "[Popularity@10] processed: 16000/190636\n",
      "[Popularity@10] processed: 18000/190636\n",
      "[Popularity@10] processed: 20000/190636\n",
      "[Popularity@10] processed: 22000/190636\n",
      "[Popularity@10] processed: 24000/190636\n",
      "[Popularity@10] processed: 26000/190636\n",
      "[Popularity@10] processed: 28000/190636\n",
      "[Popularity@10] processed: 30000/190636\n",
      "[Popularity@10] processed: 32000/190636\n",
      "[Popularity@10] processed: 34000/190636\n",
      "[Popularity@10] processed: 36000/190636\n",
      "[Popularity@10] processed: 38000/190636\n",
      "[Popularity@10] processed: 40000/190636\n",
      "[Popularity@10] processed: 42000/190636\n",
      "[Popularity@10] processed: 44000/190636\n",
      "[Popularity@10] processed: 46000/190636\n",
      "[Popularity@10] processed: 48000/190636\n",
      "[Popularity@10] processed: 50000/190636\n",
      "[Popularity@10] processed: 52000/190636\n",
      "[Popularity@10] processed: 54000/190636\n",
      "[Popularity@10] processed: 56000/190636\n",
      "[Popularity@10] processed: 58000/190636\n",
      "[Popularity@10] processed: 60000/190636\n",
      "[Popularity@10] processed: 62000/190636\n",
      "[Popularity@10] processed: 64000/190636\n",
      "[Popularity@10] processed: 66000/190636\n",
      "[Popularity@10] processed: 68000/190636\n",
      "[Popularity@10] processed: 70000/190636\n",
      "[Popularity@10] processed: 72000/190636\n",
      "[Popularity@10] processed: 74000/190636\n",
      "[Popularity@10] processed: 76000/190636\n",
      "[Popularity@10] processed: 78000/190636\n",
      "[Popularity@10] processed: 80000/190636\n",
      "[Popularity@10] processed: 82000/190636\n",
      "[Popularity@10] processed: 84000/190636\n",
      "[Popularity@10] processed: 86000/190636\n",
      "[Popularity@10] processed: 88000/190636\n",
      "[Popularity@10] processed: 90000/190636\n",
      "[Popularity@10] processed: 92000/190636\n",
      "[Popularity@10] processed: 94000/190636\n",
      "[Popularity@10] processed: 96000/190636\n",
      "[Popularity@10] processed: 98000/190636\n",
      "[Popularity@10] processed: 100000/190636\n",
      "[Popularity@10] processed: 102000/190636\n",
      "[Popularity@10] processed: 104000/190636\n",
      "[Popularity@10] processed: 106000/190636\n",
      "[Popularity@10] processed: 108000/190636\n",
      "[Popularity@10] processed: 110000/190636\n",
      "[Popularity@10] processed: 112000/190636\n",
      "[Popularity@10] processed: 114000/190636\n",
      "[Popularity@10] processed: 116000/190636\n",
      "[Popularity@10] processed: 118000/190636\n",
      "[Popularity@10] processed: 120000/190636\n",
      "[Popularity@10] processed: 122000/190636\n",
      "[Popularity@10] processed: 124000/190636\n",
      "[Popularity@10] processed: 126000/190636\n",
      "[Popularity@10] processed: 128000/190636\n",
      "[Popularity@10] processed: 130000/190636\n",
      "[Popularity@10] processed: 132000/190636\n",
      "[Popularity@10] processed: 134000/190636\n",
      "[Popularity@10] processed: 136000/190636\n",
      "[Popularity@10] processed: 138000/190636\n",
      "[Popularity@10] processed: 140000/190636\n",
      "[Popularity@10] processed: 142000/190636\n",
      "[Popularity@10] processed: 144000/190636\n",
      "[Popularity@10] processed: 146000/190636\n",
      "[Popularity@10] processed: 148000/190636\n",
      "[Popularity@10] processed: 150000/190636\n",
      "[Popularity@10] processed: 152000/190636\n",
      "[Popularity@10] processed: 154000/190636\n",
      "[Popularity@10] processed: 156000/190636\n",
      "[Popularity@10] processed: 158000/190636\n",
      "[Popularity@10] processed: 160000/190636\n",
      "[Popularity@10] processed: 162000/190636\n",
      "[Popularity@10] processed: 164000/190636\n",
      "[Popularity@10] processed: 166000/190636\n",
      "[Popularity@10] processed: 168000/190636\n",
      "[Popularity@10] processed: 170000/190636\n",
      "[Popularity@10] processed: 172000/190636\n",
      "[Popularity@10] processed: 174000/190636\n",
      "[Popularity@10] processed: 176000/190636\n",
      "[Popularity@10] processed: 178000/190636\n",
      "[Popularity@10] processed: 180000/190636\n",
      "[Popularity@10] processed: 182000/190636\n",
      "[Popularity@10] processed: 184000/190636\n",
      "[Popularity@10] processed: 186000/190636\n",
      "[Popularity@10] processed: 188000/190636\n",
      "[Popularity@10] processed: 190000/190636\n",
      "[Popularity@10] processed: 190636/190636\n",
      "[ItemCosine-RAW@10] processed: 500/190636\n",
      "[ItemCosine-RAW@10] processed: 1000/190636\n",
      "[ItemCosine-RAW@10] processed: 1500/190636\n",
      "[ItemCosine-RAW@10] processed: 2000/190636\n",
      "[ItemCosine-RAW@10] processed: 2500/190636\n",
      "[ItemCosine-RAW@10] processed: 3000/190636\n",
      "[ItemCosine-RAW@10] processed: 3500/190636\n",
      "[ItemCosine-RAW@10] processed: 4000/190636\n",
      "[ItemCosine-RAW@10] processed: 4500/190636\n",
      "[ItemCosine-RAW@10] processed: 5000/190636\n",
      "[ItemCosine-RAW@10] processed: 5500/190636\n",
      "[ItemCosine-RAW@10] processed: 6000/190636\n",
      "[ItemCosine-RAW@10] processed: 6500/190636\n",
      "[ItemCosine-RAW@10] processed: 7000/190636\n",
      "[ItemCosine-RAW@10] processed: 7500/190636\n",
      "[ItemCosine-RAW@10] processed: 8000/190636\n",
      "[ItemCosine-RAW@10] processed: 8500/190636\n",
      "[ItemCosine-RAW@10] processed: 9000/190636\n",
      "[ItemCosine-RAW@10] processed: 9500/190636\n",
      "[ItemCosine-RAW@10] processed: 10000/190636\n",
      "[ItemCosine-RAW@10] processed: 10500/190636\n",
      "[ItemCosine-RAW@10] processed: 11000/190636\n",
      "[ItemCosine-RAW@10] processed: 11500/190636\n",
      "[ItemCosine-RAW@10] processed: 12000/190636\n",
      "[ItemCosine-RAW@10] processed: 12500/190636\n",
      "[ItemCosine-RAW@10] processed: 13000/190636\n",
      "[ItemCosine-RAW@10] processed: 13500/190636\n",
      "[ItemCosine-RAW@10] processed: 14000/190636\n",
      "[ItemCosine-RAW@10] processed: 14500/190636\n",
      "[ItemCosine-RAW@10] processed: 15000/190636\n",
      "[ItemCosine-RAW@10] processed: 15500/190636\n",
      "[ItemCosine-RAW@10] processed: 16000/190636\n",
      "[ItemCosine-RAW@10] processed: 16500/190636\n",
      "[ItemCosine-RAW@10] processed: 17000/190636\n",
      "[ItemCosine-RAW@10] processed: 17500/190636\n",
      "[ItemCosine-RAW@10] processed: 18000/190636\n",
      "[ItemCosine-RAW@10] processed: 18500/190636\n",
      "[ItemCosine-RAW@10] processed: 19000/190636\n",
      "[ItemCosine-RAW@10] processed: 19500/190636\n",
      "[ItemCosine-RAW@10] processed: 20000/190636\n",
      "[ItemCosine-RAW@10] processed: 20500/190636\n",
      "[ItemCosine-RAW@10] processed: 21000/190636\n",
      "[ItemCosine-RAW@10] processed: 21500/190636\n",
      "[ItemCosine-RAW@10] processed: 22000/190636\n",
      "[ItemCosine-RAW@10] processed: 22500/190636\n",
      "[ItemCosine-RAW@10] processed: 23000/190636\n",
      "[ItemCosine-RAW@10] processed: 23500/190636\n",
      "[ItemCosine-RAW@10] processed: 24000/190636\n",
      "[ItemCosine-RAW@10] processed: 24500/190636\n",
      "[ItemCosine-RAW@10] processed: 25000/190636\n",
      "[ItemCosine-RAW@10] processed: 25500/190636\n",
      "[ItemCosine-RAW@10] processed: 26000/190636\n",
      "[ItemCosine-RAW@10] processed: 26500/190636\n",
      "[ItemCosine-RAW@10] processed: 27000/190636\n",
      "[ItemCosine-RAW@10] processed: 27500/190636\n",
      "[ItemCosine-RAW@10] processed: 28000/190636\n",
      "[ItemCosine-RAW@10] processed: 28500/190636\n",
      "[ItemCosine-RAW@10] processed: 29000/190636\n",
      "[ItemCosine-RAW@10] processed: 29500/190636\n",
      "[ItemCosine-RAW@10] processed: 30000/190636\n",
      "[ItemCosine-RAW@10] processed: 30500/190636\n",
      "[ItemCosine-RAW@10] processed: 31000/190636\n",
      "[ItemCosine-RAW@10] processed: 31500/190636\n",
      "[ItemCosine-RAW@10] processed: 32000/190636\n",
      "[ItemCosine-RAW@10] processed: 32500/190636\n",
      "[ItemCosine-RAW@10] processed: 33000/190636\n",
      "[ItemCosine-RAW@10] processed: 33500/190636\n",
      "[ItemCosine-RAW@10] processed: 34000/190636\n",
      "[ItemCosine-RAW@10] processed: 34500/190636\n",
      "[ItemCosine-RAW@10] processed: 35000/190636\n",
      "[ItemCosine-RAW@10] processed: 35500/190636\n",
      "[ItemCosine-RAW@10] processed: 36000/190636\n",
      "[ItemCosine-RAW@10] processed: 36500/190636\n",
      "[ItemCosine-RAW@10] processed: 37000/190636\n",
      "[ItemCosine-RAW@10] processed: 37500/190636\n",
      "[ItemCosine-RAW@10] processed: 38000/190636\n",
      "[ItemCosine-RAW@10] processed: 38500/190636\n",
      "[ItemCosine-RAW@10] processed: 39000/190636\n",
      "[ItemCosine-RAW@10] processed: 39500/190636\n",
      "[ItemCosine-RAW@10] processed: 40000/190636\n",
      "[ItemCosine-RAW@10] processed: 40500/190636\n",
      "[ItemCosine-RAW@10] processed: 41000/190636\n",
      "[ItemCosine-RAW@10] processed: 41500/190636\n",
      "[ItemCosine-RAW@10] processed: 42000/190636\n",
      "[ItemCosine-RAW@10] processed: 42500/190636\n",
      "[ItemCosine-RAW@10] processed: 43000/190636\n",
      "[ItemCosine-RAW@10] processed: 43500/190636\n",
      "[ItemCosine-RAW@10] processed: 44000/190636\n",
      "[ItemCosine-RAW@10] processed: 44500/190636\n",
      "[ItemCosine-RAW@10] processed: 45000/190636\n",
      "[ItemCosine-RAW@10] processed: 45500/190636\n",
      "[ItemCosine-RAW@10] processed: 46000/190636\n",
      "[ItemCosine-RAW@10] processed: 46500/190636\n",
      "[ItemCosine-RAW@10] processed: 47000/190636\n",
      "[ItemCosine-RAW@10] processed: 47500/190636\n",
      "[ItemCosine-RAW@10] processed: 48000/190636\n",
      "[ItemCosine-RAW@10] processed: 48500/190636\n",
      "[ItemCosine-RAW@10] processed: 49000/190636\n",
      "[ItemCosine-RAW@10] processed: 49500/190636\n",
      "[ItemCosine-RAW@10] processed: 50000/190636\n",
      "[ItemCosine-RAW@10] processed: 50500/190636\n",
      "[ItemCosine-RAW@10] processed: 51000/190636\n",
      "[ItemCosine-RAW@10] processed: 51500/190636\n",
      "[ItemCosine-RAW@10] processed: 52000/190636\n",
      "[ItemCosine-RAW@10] processed: 52500/190636\n",
      "[ItemCosine-RAW@10] processed: 53000/190636\n",
      "[ItemCosine-RAW@10] processed: 53500/190636\n",
      "[ItemCosine-RAW@10] processed: 54000/190636\n",
      "[ItemCosine-RAW@10] processed: 54500/190636\n",
      "[ItemCosine-RAW@10] processed: 55000/190636\n",
      "[ItemCosine-RAW@10] processed: 55500/190636\n",
      "[ItemCosine-RAW@10] processed: 56000/190636\n",
      "[ItemCosine-RAW@10] processed: 56500/190636\n",
      "[ItemCosine-RAW@10] processed: 57000/190636\n",
      "[ItemCosine-RAW@10] processed: 57500/190636\n",
      "[ItemCosine-RAW@10] processed: 58000/190636\n",
      "[ItemCosine-RAW@10] processed: 58500/190636\n",
      "[ItemCosine-RAW@10] processed: 59000/190636\n",
      "[ItemCosine-RAW@10] processed: 59500/190636\n",
      "[ItemCosine-RAW@10] processed: 60000/190636\n",
      "[ItemCosine-RAW@10] processed: 60500/190636\n",
      "[ItemCosine-RAW@10] processed: 61000/190636\n",
      "[ItemCosine-RAW@10] processed: 61500/190636\n",
      "[ItemCosine-RAW@10] processed: 62000/190636\n",
      "[ItemCosine-RAW@10] processed: 62500/190636\n",
      "[ItemCosine-RAW@10] processed: 63000/190636\n",
      "[ItemCosine-RAW@10] processed: 63500/190636\n",
      "[ItemCosine-RAW@10] processed: 64000/190636\n",
      "[ItemCosine-RAW@10] processed: 64500/190636\n",
      "[ItemCosine-RAW@10] processed: 65000/190636\n",
      "[ItemCosine-RAW@10] processed: 65500/190636\n",
      "[ItemCosine-RAW@10] processed: 66000/190636\n",
      "[ItemCosine-RAW@10] processed: 66500/190636\n",
      "[ItemCosine-RAW@10] processed: 67000/190636\n",
      "[ItemCosine-RAW@10] processed: 67500/190636\n",
      "[ItemCosine-RAW@10] processed: 68000/190636\n",
      "[ItemCosine-RAW@10] processed: 68500/190636\n",
      "[ItemCosine-RAW@10] processed: 69000/190636\n",
      "[ItemCosine-RAW@10] processed: 69500/190636\n",
      "[ItemCosine-RAW@10] processed: 70000/190636\n",
      "[ItemCosine-RAW@10] processed: 70500/190636\n",
      "[ItemCosine-RAW@10] processed: 71000/190636\n",
      "[ItemCosine-RAW@10] processed: 71500/190636\n",
      "[ItemCosine-RAW@10] processed: 72000/190636\n",
      "[ItemCosine-RAW@10] processed: 72500/190636\n",
      "[ItemCosine-RAW@10] processed: 73000/190636\n",
      "[ItemCosine-RAW@10] processed: 73500/190636\n",
      "[ItemCosine-RAW@10] processed: 74000/190636\n",
      "[ItemCosine-RAW@10] processed: 74500/190636\n",
      "[ItemCosine-RAW@10] processed: 75000/190636\n",
      "[ItemCosine-RAW@10] processed: 75500/190636\n",
      "[ItemCosine-RAW@10] processed: 76000/190636\n",
      "[ItemCosine-RAW@10] processed: 76500/190636\n",
      "[ItemCosine-RAW@10] processed: 77000/190636\n",
      "[ItemCosine-RAW@10] processed: 77500/190636\n",
      "[ItemCosine-RAW@10] processed: 78000/190636\n",
      "[ItemCosine-RAW@10] processed: 78500/190636\n",
      "[ItemCosine-RAW@10] processed: 79000/190636\n",
      "[ItemCosine-RAW@10] processed: 79500/190636\n",
      "[ItemCosine-RAW@10] processed: 80000/190636\n",
      "[ItemCosine-RAW@10] processed: 80500/190636\n",
      "[ItemCosine-RAW@10] processed: 81000/190636\n",
      "[ItemCosine-RAW@10] processed: 81500/190636\n",
      "[ItemCosine-RAW@10] processed: 82000/190636\n",
      "[ItemCosine-RAW@10] processed: 82500/190636\n",
      "[ItemCosine-RAW@10] processed: 83000/190636\n",
      "[ItemCosine-RAW@10] processed: 83500/190636\n",
      "[ItemCosine-RAW@10] processed: 84000/190636\n",
      "[ItemCosine-RAW@10] processed: 84500/190636\n",
      "[ItemCosine-RAW@10] processed: 85000/190636\n",
      "[ItemCosine-RAW@10] processed: 85500/190636\n",
      "[ItemCosine-RAW@10] processed: 86000/190636\n",
      "[ItemCosine-RAW@10] processed: 86500/190636\n",
      "[ItemCosine-RAW@10] processed: 87000/190636\n",
      "[ItemCosine-RAW@10] processed: 87500/190636\n",
      "[ItemCosine-RAW@10] processed: 88000/190636\n",
      "[ItemCosine-RAW@10] processed: 88500/190636\n",
      "[ItemCosine-RAW@10] processed: 89000/190636\n",
      "[ItemCosine-RAW@10] processed: 89500/190636\n",
      "[ItemCosine-RAW@10] processed: 90000/190636\n",
      "[ItemCosine-RAW@10] processed: 90500/190636\n",
      "[ItemCosine-RAW@10] processed: 91000/190636\n",
      "[ItemCosine-RAW@10] processed: 91500/190636\n",
      "[ItemCosine-RAW@10] processed: 92000/190636\n",
      "[ItemCosine-RAW@10] processed: 92500/190636\n",
      "[ItemCosine-RAW@10] processed: 93000/190636\n",
      "[ItemCosine-RAW@10] processed: 93500/190636\n",
      "[ItemCosine-RAW@10] processed: 94000/190636\n",
      "[ItemCosine-RAW@10] processed: 94500/190636\n",
      "[ItemCosine-RAW@10] processed: 95000/190636\n",
      "[ItemCosine-RAW@10] processed: 95500/190636\n",
      "[ItemCosine-RAW@10] processed: 96000/190636\n",
      "[ItemCosine-RAW@10] processed: 96500/190636\n",
      "[ItemCosine-RAW@10] processed: 97000/190636\n",
      "[ItemCosine-RAW@10] processed: 97500/190636\n",
      "[ItemCosine-RAW@10] processed: 98000/190636\n",
      "[ItemCosine-RAW@10] processed: 98500/190636\n",
      "[ItemCosine-RAW@10] processed: 99000/190636\n",
      "[ItemCosine-RAW@10] processed: 99500/190636\n",
      "[ItemCosine-RAW@10] processed: 100000/190636\n",
      "[ItemCosine-RAW@10] processed: 100500/190636\n",
      "[ItemCosine-RAW@10] processed: 101000/190636\n",
      "[ItemCosine-RAW@10] processed: 101500/190636\n",
      "[ItemCosine-RAW@10] processed: 102000/190636\n",
      "[ItemCosine-RAW@10] processed: 102500/190636\n",
      "[ItemCosine-RAW@10] processed: 103000/190636\n",
      "[ItemCosine-RAW@10] processed: 103500/190636\n",
      "[ItemCosine-RAW@10] processed: 104000/190636\n",
      "[ItemCosine-RAW@10] processed: 104500/190636\n",
      "[ItemCosine-RAW@10] processed: 105000/190636\n",
      "[ItemCosine-RAW@10] processed: 105500/190636\n",
      "[ItemCosine-RAW@10] processed: 106000/190636\n",
      "[ItemCosine-RAW@10] processed: 106500/190636\n",
      "[ItemCosine-RAW@10] processed: 107000/190636\n",
      "[ItemCosine-RAW@10] processed: 107500/190636\n",
      "[ItemCosine-RAW@10] processed: 108000/190636\n",
      "[ItemCosine-RAW@10] processed: 108500/190636\n",
      "[ItemCosine-RAW@10] processed: 109000/190636\n",
      "[ItemCosine-RAW@10] processed: 109500/190636\n",
      "[ItemCosine-RAW@10] processed: 110000/190636\n",
      "[ItemCosine-RAW@10] processed: 110500/190636\n",
      "[ItemCosine-RAW@10] processed: 111000/190636\n",
      "[ItemCosine-RAW@10] processed: 111500/190636\n",
      "[ItemCosine-RAW@10] processed: 112000/190636\n",
      "[ItemCosine-RAW@10] processed: 112500/190636\n",
      "[ItemCosine-RAW@10] processed: 113000/190636\n",
      "[ItemCosine-RAW@10] processed: 113500/190636\n",
      "[ItemCosine-RAW@10] processed: 114000/190636\n",
      "[ItemCosine-RAW@10] processed: 114500/190636\n",
      "[ItemCosine-RAW@10] processed: 115000/190636\n",
      "[ItemCosine-RAW@10] processed: 115500/190636\n",
      "[ItemCosine-RAW@10] processed: 116000/190636\n",
      "[ItemCosine-RAW@10] processed: 116500/190636\n",
      "[ItemCosine-RAW@10] processed: 117000/190636\n",
      "[ItemCosine-RAW@10] processed: 117500/190636\n",
      "[ItemCosine-RAW@10] processed: 118000/190636\n",
      "[ItemCosine-RAW@10] processed: 118500/190636\n",
      "[ItemCosine-RAW@10] processed: 119000/190636\n",
      "[ItemCosine-RAW@10] processed: 119500/190636\n",
      "[ItemCosine-RAW@10] processed: 120000/190636\n",
      "[ItemCosine-RAW@10] processed: 120500/190636\n",
      "[ItemCosine-RAW@10] processed: 121000/190636\n",
      "[ItemCosine-RAW@10] processed: 121500/190636\n",
      "[ItemCosine-RAW@10] processed: 122000/190636\n",
      "[ItemCosine-RAW@10] processed: 122500/190636\n",
      "[ItemCosine-RAW@10] processed: 123000/190636\n",
      "[ItemCosine-RAW@10] processed: 123500/190636\n",
      "[ItemCosine-RAW@10] processed: 124000/190636\n",
      "[ItemCosine-RAW@10] processed: 124500/190636\n",
      "[ItemCosine-RAW@10] processed: 125000/190636\n",
      "[ItemCosine-RAW@10] processed: 125500/190636\n",
      "[ItemCosine-RAW@10] processed: 126000/190636\n",
      "[ItemCosine-RAW@10] processed: 126500/190636\n",
      "[ItemCosine-RAW@10] processed: 127000/190636\n",
      "[ItemCosine-RAW@10] processed: 127500/190636\n",
      "[ItemCosine-RAW@10] processed: 128000/190636\n",
      "[ItemCosine-RAW@10] processed: 128500/190636\n",
      "[ItemCosine-RAW@10] processed: 129000/190636\n",
      "[ItemCosine-RAW@10] processed: 129500/190636\n",
      "[ItemCosine-RAW@10] processed: 130000/190636\n",
      "[ItemCosine-RAW@10] processed: 130500/190636\n",
      "[ItemCosine-RAW@10] processed: 131000/190636\n",
      "[ItemCosine-RAW@10] processed: 131500/190636\n",
      "[ItemCosine-RAW@10] processed: 132000/190636\n",
      "[ItemCosine-RAW@10] processed: 132500/190636\n",
      "[ItemCosine-RAW@10] processed: 133000/190636\n",
      "[ItemCosine-RAW@10] processed: 133500/190636\n",
      "[ItemCosine-RAW@10] processed: 134000/190636\n",
      "[ItemCosine-RAW@10] processed: 134500/190636\n",
      "[ItemCosine-RAW@10] processed: 135000/190636\n",
      "[ItemCosine-RAW@10] processed: 135500/190636\n",
      "[ItemCosine-RAW@10] processed: 136000/190636\n",
      "[ItemCosine-RAW@10] processed: 136500/190636\n",
      "[ItemCosine-RAW@10] processed: 137000/190636\n",
      "[ItemCosine-RAW@10] processed: 137500/190636\n",
      "[ItemCosine-RAW@10] processed: 138000/190636\n",
      "[ItemCosine-RAW@10] processed: 138500/190636\n",
      "[ItemCosine-RAW@10] processed: 139000/190636\n",
      "[ItemCosine-RAW@10] processed: 139500/190636\n",
      "[ItemCosine-RAW@10] processed: 140000/190636\n",
      "[ItemCosine-RAW@10] processed: 140500/190636\n",
      "[ItemCosine-RAW@10] processed: 141000/190636\n",
      "[ItemCosine-RAW@10] processed: 141500/190636\n",
      "[ItemCosine-RAW@10] processed: 142000/190636\n",
      "[ItemCosine-RAW@10] processed: 142500/190636\n",
      "[ItemCosine-RAW@10] processed: 143000/190636\n",
      "[ItemCosine-RAW@10] processed: 143500/190636\n",
      "[ItemCosine-RAW@10] processed: 144000/190636\n",
      "[ItemCosine-RAW@10] processed: 144500/190636\n",
      "[ItemCosine-RAW@10] processed: 145000/190636\n",
      "[ItemCosine-RAW@10] processed: 145500/190636\n",
      "[ItemCosine-RAW@10] processed: 146000/190636\n",
      "[ItemCosine-RAW@10] processed: 146500/190636\n",
      "[ItemCosine-RAW@10] processed: 147000/190636\n",
      "[ItemCosine-RAW@10] processed: 147500/190636\n",
      "[ItemCosine-RAW@10] processed: 148000/190636\n",
      "[ItemCosine-RAW@10] processed: 148500/190636\n",
      "[ItemCosine-RAW@10] processed: 149000/190636\n",
      "[ItemCosine-RAW@10] processed: 149500/190636\n",
      "[ItemCosine-RAW@10] processed: 150000/190636\n",
      "[ItemCosine-RAW@10] processed: 150500/190636\n",
      "[ItemCosine-RAW@10] processed: 151000/190636\n",
      "[ItemCosine-RAW@10] processed: 151500/190636\n",
      "[ItemCosine-RAW@10] processed: 152000/190636\n",
      "[ItemCosine-RAW@10] processed: 152500/190636\n",
      "[ItemCosine-RAW@10] processed: 153000/190636\n",
      "[ItemCosine-RAW@10] processed: 153500/190636\n",
      "[ItemCosine-RAW@10] processed: 154000/190636\n",
      "[ItemCosine-RAW@10] processed: 154500/190636\n",
      "[ItemCosine-RAW@10] processed: 155000/190636\n",
      "[ItemCosine-RAW@10] processed: 155500/190636\n",
      "[ItemCosine-RAW@10] processed: 156000/190636\n",
      "[ItemCosine-RAW@10] processed: 156500/190636\n",
      "[ItemCosine-RAW@10] processed: 157000/190636\n",
      "[ItemCosine-RAW@10] processed: 157500/190636\n",
      "[ItemCosine-RAW@10] processed: 158000/190636\n",
      "[ItemCosine-RAW@10] processed: 158500/190636\n",
      "[ItemCosine-RAW@10] processed: 159000/190636\n",
      "[ItemCosine-RAW@10] processed: 159500/190636\n",
      "[ItemCosine-RAW@10] processed: 160000/190636\n",
      "[ItemCosine-RAW@10] processed: 160500/190636\n",
      "[ItemCosine-RAW@10] processed: 161000/190636\n",
      "[ItemCosine-RAW@10] processed: 161500/190636\n",
      "[ItemCosine-RAW@10] processed: 162000/190636\n",
      "[ItemCosine-RAW@10] processed: 162500/190636\n",
      "[ItemCosine-RAW@10] processed: 163000/190636\n",
      "[ItemCosine-RAW@10] processed: 163500/190636\n",
      "[ItemCosine-RAW@10] processed: 164000/190636\n",
      "[ItemCosine-RAW@10] processed: 164500/190636\n",
      "[ItemCosine-RAW@10] processed: 165000/190636\n",
      "[ItemCosine-RAW@10] processed: 165500/190636\n",
      "[ItemCosine-RAW@10] processed: 166000/190636\n",
      "[ItemCosine-RAW@10] processed: 166500/190636\n",
      "[ItemCosine-RAW@10] processed: 167000/190636\n",
      "[ItemCosine-RAW@10] processed: 167500/190636\n",
      "[ItemCosine-RAW@10] processed: 168000/190636\n",
      "[ItemCosine-RAW@10] processed: 168500/190636\n",
      "[ItemCosine-RAW@10] processed: 169000/190636\n",
      "[ItemCosine-RAW@10] processed: 169500/190636\n",
      "[ItemCosine-RAW@10] processed: 170000/190636\n",
      "[ItemCosine-RAW@10] processed: 170500/190636\n",
      "[ItemCosine-RAW@10] processed: 171000/190636\n",
      "[ItemCosine-RAW@10] processed: 171500/190636\n",
      "[ItemCosine-RAW@10] processed: 172000/190636\n",
      "[ItemCosine-RAW@10] processed: 172500/190636\n",
      "[ItemCosine-RAW@10] processed: 173000/190636\n",
      "[ItemCosine-RAW@10] processed: 173500/190636\n",
      "[ItemCosine-RAW@10] processed: 174000/190636\n",
      "[ItemCosine-RAW@10] processed: 174500/190636\n",
      "[ItemCosine-RAW@10] processed: 175000/190636\n",
      "[ItemCosine-RAW@10] processed: 175500/190636\n",
      "[ItemCosine-RAW@10] processed: 176000/190636\n",
      "[ItemCosine-RAW@10] processed: 176500/190636\n",
      "[ItemCosine-RAW@10] processed: 177000/190636\n",
      "[ItemCosine-RAW@10] processed: 177500/190636\n",
      "[ItemCosine-RAW@10] processed: 178000/190636\n",
      "[ItemCosine-RAW@10] processed: 178500/190636\n",
      "[ItemCosine-RAW@10] processed: 179000/190636\n",
      "[ItemCosine-RAW@10] processed: 179500/190636\n",
      "[ItemCosine-RAW@10] processed: 180000/190636\n",
      "[ItemCosine-RAW@10] processed: 180500/190636\n",
      "[ItemCosine-RAW@10] processed: 181000/190636\n",
      "[ItemCosine-RAW@10] processed: 181500/190636\n",
      "[ItemCosine-RAW@10] processed: 182000/190636\n",
      "[ItemCosine-RAW@10] processed: 182500/190636\n",
      "[ItemCosine-RAW@10] processed: 183000/190636\n",
      "[ItemCosine-RAW@10] processed: 183500/190636\n",
      "[ItemCosine-RAW@10] processed: 184000/190636\n",
      "[ItemCosine-RAW@10] processed: 184500/190636\n",
      "[ItemCosine-RAW@10] processed: 185000/190636\n",
      "[ItemCosine-RAW@10] processed: 185500/190636\n",
      "[ItemCosine-RAW@10] processed: 186000/190636\n",
      "[ItemCosine-RAW@10] processed: 186500/190636\n",
      "[ItemCosine-RAW@10] processed: 187000/190636\n",
      "[ItemCosine-RAW@10] processed: 187500/190636\n",
      "[ItemCosine-RAW@10] processed: 188000/190636\n",
      "[ItemCosine-RAW@10] processed: 188500/190636\n",
      "[ItemCosine-RAW@10] processed: 189000/190636\n",
      "[ItemCosine-RAW@10] processed: 189500/190636\n",
      "[ItemCosine-RAW@10] processed: 190000/190636\n",
      "[ItemCosine-RAW@10] processed: 190500/190636\n",
      "[ItemCosine-RAW@10] processed: 190636/190636\n",
      "[ItemCosine-TFIDF@10] processed: 500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 1000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 1500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 2000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 2500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 3000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 3500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 4000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 4500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 5000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 5500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 6000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 6500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 7000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 7500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 8000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 8500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 9000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 9500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 10000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 10500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 11000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 11500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 12000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 12500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 13000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 13500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 14000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 14500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 15000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 15500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 16000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 16500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 17000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 17500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 18000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 18500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 19000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 19500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 20000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 20500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 21000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 21500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 22000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 22500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 23000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 23500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 24000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 24500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 25000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 25500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 26000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 26500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 27000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 27500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 28000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 28500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 29000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 29500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 30000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 30500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 31000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 31500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 32000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 32500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 33000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 33500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 34000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 34500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 35000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 35500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 36000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 36500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 37000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 37500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 38000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 38500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 39000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 39500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 40000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 40500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 41000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 41500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 42000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 42500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 43000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 43500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 44000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 44500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 45000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 45500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 46000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 46500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 47000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 47500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 48000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 48500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 49000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 49500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 50000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 50500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 51000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 51500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 52000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 52500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 53000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 53500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 54000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 54500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 55000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 55500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 56000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 56500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 57000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 57500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 58000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 58500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 59000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 59500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 60000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 60500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 61000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 61500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 62000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 62500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 63000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 63500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 64000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 64500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 65000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 65500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 66000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 66500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 67000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 67500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 68000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 68500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 69000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 69500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 70000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 70500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 71000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 71500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 72000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 72500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 73000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 73500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 74000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 74500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 75000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 75500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 76000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 76500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 77000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 77500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 78000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 78500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 79000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 79500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 80000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 80500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 81000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 81500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 82000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 82500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 83000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 83500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 84000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 84500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 85000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 85500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 86000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 86500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 87000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 87500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 88000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 88500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 89000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 89500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 90000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 90500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 91000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 91500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 92000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 92500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 93000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 93500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 94000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 94500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 95000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 95500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 96000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 96500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 97000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 97500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 98000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 98500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 99000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 99500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 100000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 100500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 101000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 101500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 102000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 102500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 103000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 103500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 104000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 104500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 105000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 105500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 106000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 106500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 107000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 107500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 108000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 108500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 109000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 109500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 110000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 110500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 111000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 111500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 112000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 112500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 113000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 113500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 114000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 114500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 115000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 115500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 116000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 116500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 117000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 117500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 118000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 118500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 119000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 119500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 120000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 120500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 121000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 121500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 122000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 122500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 123000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 123500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 124000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 124500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 125000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 125500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 126000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 126500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 127000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 127500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 128000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 128500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 129000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 129500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 130000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 130500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 131000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 131500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 132000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 132500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 133000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 133500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 134000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 134500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 135000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 135500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 136000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 136500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 137000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 137500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 138000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 138500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 139000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 139500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 140000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 140500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 141000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 141500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 142000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 142500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 143000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 143500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 144000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 144500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 145000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 145500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 146000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 146500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 147000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 147500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 148000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 148500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 149000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 149500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 150000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 150500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 151000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 151500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 152000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 152500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 153000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 153500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 154000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 154500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 155000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 155500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 156000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 156500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 157000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 157500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 158000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 158500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 159000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 159500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 160000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 160500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 161000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 161500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 162000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 162500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 163000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 163500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 164000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 164500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 165000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 165500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 166000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 166500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 167000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 167500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 168000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 168500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 169000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 169500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 170000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 170500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 171000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 171500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 172000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 172500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 173000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 173500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 174000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 174500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 175000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 175500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 176000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 176500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 177000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 177500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 178000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 178500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 179000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 179500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 180000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 180500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 181000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 181500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 182000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 182500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 183000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 183500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 184000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 184500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 185000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 185500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 186000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 186500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 187000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 187500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 188000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 188500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 189000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 189500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 190000/190636\n",
      "[ItemCosine-TFIDF@10] processed: 190500/190636\n",
      "[ItemCosine-TFIDF@10] processed: 190636/190636\n",
      "[ItemCosine-BM25@10] processed: 500/190636\n",
      "[ItemCosine-BM25@10] processed: 1000/190636\n",
      "[ItemCosine-BM25@10] processed: 1500/190636\n",
      "[ItemCosine-BM25@10] processed: 2000/190636\n",
      "[ItemCosine-BM25@10] processed: 2500/190636\n",
      "[ItemCosine-BM25@10] processed: 3000/190636\n",
      "[ItemCosine-BM25@10] processed: 3500/190636\n",
      "[ItemCosine-BM25@10] processed: 4000/190636\n",
      "[ItemCosine-BM25@10] processed: 4500/190636\n",
      "[ItemCosine-BM25@10] processed: 5000/190636\n",
      "[ItemCosine-BM25@10] processed: 5500/190636\n",
      "[ItemCosine-BM25@10] processed: 6000/190636\n",
      "[ItemCosine-BM25@10] processed: 6500/190636\n",
      "[ItemCosine-BM25@10] processed: 7000/190636\n",
      "[ItemCosine-BM25@10] processed: 7500/190636\n",
      "[ItemCosine-BM25@10] processed: 8000/190636\n",
      "[ItemCosine-BM25@10] processed: 8500/190636\n",
      "[ItemCosine-BM25@10] processed: 9000/190636\n",
      "[ItemCosine-BM25@10] processed: 9500/190636\n",
      "[ItemCosine-BM25@10] processed: 10000/190636\n",
      "[ItemCosine-BM25@10] processed: 10500/190636\n",
      "[ItemCosine-BM25@10] processed: 11000/190636\n",
      "[ItemCosine-BM25@10] processed: 11500/190636\n",
      "[ItemCosine-BM25@10] processed: 12000/190636\n",
      "[ItemCosine-BM25@10] processed: 12500/190636\n",
      "[ItemCosine-BM25@10] processed: 13000/190636\n",
      "[ItemCosine-BM25@10] processed: 13500/190636\n",
      "[ItemCosine-BM25@10] processed: 14000/190636\n",
      "[ItemCosine-BM25@10] processed: 14500/190636\n",
      "[ItemCosine-BM25@10] processed: 15000/190636\n",
      "[ItemCosine-BM25@10] processed: 15500/190636\n",
      "[ItemCosine-BM25@10] processed: 16000/190636\n",
      "[ItemCosine-BM25@10] processed: 16500/190636\n",
      "[ItemCosine-BM25@10] processed: 17000/190636\n",
      "[ItemCosine-BM25@10] processed: 17500/190636\n",
      "[ItemCosine-BM25@10] processed: 18000/190636\n",
      "[ItemCosine-BM25@10] processed: 18500/190636\n",
      "[ItemCosine-BM25@10] processed: 19000/190636\n",
      "[ItemCosine-BM25@10] processed: 19500/190636\n",
      "[ItemCosine-BM25@10] processed: 20000/190636\n",
      "[ItemCosine-BM25@10] processed: 20500/190636\n",
      "[ItemCosine-BM25@10] processed: 21000/190636\n",
      "[ItemCosine-BM25@10] processed: 21500/190636\n",
      "[ItemCosine-BM25@10] processed: 22000/190636\n",
      "[ItemCosine-BM25@10] processed: 22500/190636\n",
      "[ItemCosine-BM25@10] processed: 23000/190636\n",
      "[ItemCosine-BM25@10] processed: 23500/190636\n",
      "[ItemCosine-BM25@10] processed: 24000/190636\n",
      "[ItemCosine-BM25@10] processed: 24500/190636\n",
      "[ItemCosine-BM25@10] processed: 25000/190636\n",
      "[ItemCosine-BM25@10] processed: 25500/190636\n",
      "[ItemCosine-BM25@10] processed: 26000/190636\n",
      "[ItemCosine-BM25@10] processed: 26500/190636\n",
      "[ItemCosine-BM25@10] processed: 27000/190636\n",
      "[ItemCosine-BM25@10] processed: 27500/190636\n",
      "[ItemCosine-BM25@10] processed: 28000/190636\n",
      "[ItemCosine-BM25@10] processed: 28500/190636\n",
      "[ItemCosine-BM25@10] processed: 29000/190636\n",
      "[ItemCosine-BM25@10] processed: 29500/190636\n",
      "[ItemCosine-BM25@10] processed: 30000/190636\n",
      "[ItemCosine-BM25@10] processed: 30500/190636\n",
      "[ItemCosine-BM25@10] processed: 31000/190636\n",
      "[ItemCosine-BM25@10] processed: 31500/190636\n",
      "[ItemCosine-BM25@10] processed: 32000/190636\n",
      "[ItemCosine-BM25@10] processed: 32500/190636\n",
      "[ItemCosine-BM25@10] processed: 33000/190636\n",
      "[ItemCosine-BM25@10] processed: 33500/190636\n",
      "[ItemCosine-BM25@10] processed: 34000/190636\n",
      "[ItemCosine-BM25@10] processed: 34500/190636\n",
      "[ItemCosine-BM25@10] processed: 35000/190636\n",
      "[ItemCosine-BM25@10] processed: 35500/190636\n",
      "[ItemCosine-BM25@10] processed: 36000/190636\n",
      "[ItemCosine-BM25@10] processed: 36500/190636\n",
      "[ItemCosine-BM25@10] processed: 37000/190636\n",
      "[ItemCosine-BM25@10] processed: 37500/190636\n",
      "[ItemCosine-BM25@10] processed: 38000/190636\n",
      "[ItemCosine-BM25@10] processed: 38500/190636\n",
      "[ItemCosine-BM25@10] processed: 39000/190636\n",
      "[ItemCosine-BM25@10] processed: 39500/190636\n",
      "[ItemCosine-BM25@10] processed: 40000/190636\n",
      "[ItemCosine-BM25@10] processed: 40500/190636\n",
      "[ItemCosine-BM25@10] processed: 41000/190636\n",
      "[ItemCosine-BM25@10] processed: 41500/190636\n",
      "[ItemCosine-BM25@10] processed: 42000/190636\n",
      "[ItemCosine-BM25@10] processed: 42500/190636\n",
      "[ItemCosine-BM25@10] processed: 43000/190636\n",
      "[ItemCosine-BM25@10] processed: 43500/190636\n",
      "[ItemCosine-BM25@10] processed: 44000/190636\n",
      "[ItemCosine-BM25@10] processed: 44500/190636\n",
      "[ItemCosine-BM25@10] processed: 45000/190636\n",
      "[ItemCosine-BM25@10] processed: 45500/190636\n",
      "[ItemCosine-BM25@10] processed: 46000/190636\n",
      "[ItemCosine-BM25@10] processed: 46500/190636\n",
      "[ItemCosine-BM25@10] processed: 47000/190636\n",
      "[ItemCosine-BM25@10] processed: 47500/190636\n",
      "[ItemCosine-BM25@10] processed: 48000/190636\n",
      "[ItemCosine-BM25@10] processed: 48500/190636\n",
      "[ItemCosine-BM25@10] processed: 49000/190636\n",
      "[ItemCosine-BM25@10] processed: 49500/190636\n",
      "[ItemCosine-BM25@10] processed: 50000/190636\n",
      "[ItemCosine-BM25@10] processed: 50500/190636\n",
      "[ItemCosine-BM25@10] processed: 51000/190636\n",
      "[ItemCosine-BM25@10] processed: 51500/190636\n",
      "[ItemCosine-BM25@10] processed: 52000/190636\n",
      "[ItemCosine-BM25@10] processed: 52500/190636\n",
      "[ItemCosine-BM25@10] processed: 53000/190636\n",
      "[ItemCosine-BM25@10] processed: 53500/190636\n",
      "[ItemCosine-BM25@10] processed: 54000/190636\n",
      "[ItemCosine-BM25@10] processed: 54500/190636\n",
      "[ItemCosine-BM25@10] processed: 55000/190636\n",
      "[ItemCosine-BM25@10] processed: 55500/190636\n",
      "[ItemCosine-BM25@10] processed: 56000/190636\n",
      "[ItemCosine-BM25@10] processed: 56500/190636\n",
      "[ItemCosine-BM25@10] processed: 57000/190636\n",
      "[ItemCosine-BM25@10] processed: 57500/190636\n",
      "[ItemCosine-BM25@10] processed: 58000/190636\n",
      "[ItemCosine-BM25@10] processed: 58500/190636\n",
      "[ItemCosine-BM25@10] processed: 59000/190636\n",
      "[ItemCosine-BM25@10] processed: 59500/190636\n",
      "[ItemCosine-BM25@10] processed: 60000/190636\n",
      "[ItemCosine-BM25@10] processed: 60500/190636\n",
      "[ItemCosine-BM25@10] processed: 61000/190636\n",
      "[ItemCosine-BM25@10] processed: 61500/190636\n",
      "[ItemCosine-BM25@10] processed: 62000/190636\n",
      "[ItemCosine-BM25@10] processed: 62500/190636\n",
      "[ItemCosine-BM25@10] processed: 63000/190636\n",
      "[ItemCosine-BM25@10] processed: 63500/190636\n",
      "[ItemCosine-BM25@10] processed: 64000/190636\n",
      "[ItemCosine-BM25@10] processed: 64500/190636\n",
      "[ItemCosine-BM25@10] processed: 65000/190636\n",
      "[ItemCosine-BM25@10] processed: 65500/190636\n",
      "[ItemCosine-BM25@10] processed: 66000/190636\n",
      "[ItemCosine-BM25@10] processed: 66500/190636\n",
      "[ItemCosine-BM25@10] processed: 67000/190636\n",
      "[ItemCosine-BM25@10] processed: 67500/190636\n",
      "[ItemCosine-BM25@10] processed: 68000/190636\n",
      "[ItemCosine-BM25@10] processed: 68500/190636\n",
      "[ItemCosine-BM25@10] processed: 69000/190636\n",
      "[ItemCosine-BM25@10] processed: 69500/190636\n",
      "[ItemCosine-BM25@10] processed: 70000/190636\n",
      "[ItemCosine-BM25@10] processed: 70500/190636\n",
      "[ItemCosine-BM25@10] processed: 71000/190636\n",
      "[ItemCosine-BM25@10] processed: 71500/190636\n",
      "[ItemCosine-BM25@10] processed: 72000/190636\n",
      "[ItemCosine-BM25@10] processed: 72500/190636\n",
      "[ItemCosine-BM25@10] processed: 73000/190636\n",
      "[ItemCosine-BM25@10] processed: 73500/190636\n",
      "[ItemCosine-BM25@10] processed: 74000/190636\n",
      "[ItemCosine-BM25@10] processed: 74500/190636\n",
      "[ItemCosine-BM25@10] processed: 75000/190636\n",
      "[ItemCosine-BM25@10] processed: 75500/190636\n",
      "[ItemCosine-BM25@10] processed: 76000/190636\n",
      "[ItemCosine-BM25@10] processed: 76500/190636\n",
      "[ItemCosine-BM25@10] processed: 77000/190636\n",
      "[ItemCosine-BM25@10] processed: 77500/190636\n",
      "[ItemCosine-BM25@10] processed: 78000/190636\n",
      "[ItemCosine-BM25@10] processed: 78500/190636\n",
      "[ItemCosine-BM25@10] processed: 79000/190636\n",
      "[ItemCosine-BM25@10] processed: 79500/190636\n",
      "[ItemCosine-BM25@10] processed: 80000/190636\n",
      "[ItemCosine-BM25@10] processed: 80500/190636\n",
      "[ItemCosine-BM25@10] processed: 81000/190636\n",
      "[ItemCosine-BM25@10] processed: 81500/190636\n",
      "[ItemCosine-BM25@10] processed: 82000/190636\n",
      "[ItemCosine-BM25@10] processed: 82500/190636\n",
      "[ItemCosine-BM25@10] processed: 83000/190636\n",
      "[ItemCosine-BM25@10] processed: 83500/190636\n",
      "[ItemCosine-BM25@10] processed: 84000/190636\n",
      "[ItemCosine-BM25@10] processed: 84500/190636\n",
      "[ItemCosine-BM25@10] processed: 85000/190636\n",
      "[ItemCosine-BM25@10] processed: 85500/190636\n",
      "[ItemCosine-BM25@10] processed: 86000/190636\n",
      "[ItemCosine-BM25@10] processed: 86500/190636\n",
      "[ItemCosine-BM25@10] processed: 87000/190636\n",
      "[ItemCosine-BM25@10] processed: 87500/190636\n",
      "[ItemCosine-BM25@10] processed: 88000/190636\n",
      "[ItemCosine-BM25@10] processed: 88500/190636\n",
      "[ItemCosine-BM25@10] processed: 89000/190636\n",
      "[ItemCosine-BM25@10] processed: 89500/190636\n",
      "[ItemCosine-BM25@10] processed: 90000/190636\n",
      "[ItemCosine-BM25@10] processed: 90500/190636\n",
      "[ItemCosine-BM25@10] processed: 91000/190636\n",
      "[ItemCosine-BM25@10] processed: 91500/190636\n",
      "[ItemCosine-BM25@10] processed: 92000/190636\n",
      "[ItemCosine-BM25@10] processed: 92500/190636\n",
      "[ItemCosine-BM25@10] processed: 93000/190636\n",
      "[ItemCosine-BM25@10] processed: 93500/190636\n",
      "[ItemCosine-BM25@10] processed: 94000/190636\n",
      "[ItemCosine-BM25@10] processed: 94500/190636\n",
      "[ItemCosine-BM25@10] processed: 95000/190636\n",
      "[ItemCosine-BM25@10] processed: 95500/190636\n",
      "[ItemCosine-BM25@10] processed: 96000/190636\n",
      "[ItemCosine-BM25@10] processed: 96500/190636\n",
      "[ItemCosine-BM25@10] processed: 97000/190636\n",
      "[ItemCosine-BM25@10] processed: 97500/190636\n",
      "[ItemCosine-BM25@10] processed: 98000/190636\n",
      "[ItemCosine-BM25@10] processed: 98500/190636\n",
      "[ItemCosine-BM25@10] processed: 99000/190636\n",
      "[ItemCosine-BM25@10] processed: 99500/190636\n",
      "[ItemCosine-BM25@10] processed: 100000/190636\n",
      "[ItemCosine-BM25@10] processed: 100500/190636\n",
      "[ItemCosine-BM25@10] processed: 101000/190636\n",
      "[ItemCosine-BM25@10] processed: 101500/190636\n",
      "[ItemCosine-BM25@10] processed: 102000/190636\n",
      "[ItemCosine-BM25@10] processed: 102500/190636\n",
      "[ItemCosine-BM25@10] processed: 103000/190636\n",
      "[ItemCosine-BM25@10] processed: 103500/190636\n",
      "[ItemCosine-BM25@10] processed: 104000/190636\n",
      "[ItemCosine-BM25@10] processed: 104500/190636\n",
      "[ItemCosine-BM25@10] processed: 105000/190636\n",
      "[ItemCosine-BM25@10] processed: 105500/190636\n",
      "[ItemCosine-BM25@10] processed: 106000/190636\n",
      "[ItemCosine-BM25@10] processed: 106500/190636\n",
      "[ItemCosine-BM25@10] processed: 107000/190636\n",
      "[ItemCosine-BM25@10] processed: 107500/190636\n",
      "[ItemCosine-BM25@10] processed: 108000/190636\n",
      "[ItemCosine-BM25@10] processed: 108500/190636\n",
      "[ItemCosine-BM25@10] processed: 109000/190636\n",
      "[ItemCosine-BM25@10] processed: 109500/190636\n",
      "[ItemCosine-BM25@10] processed: 110000/190636\n",
      "[ItemCosine-BM25@10] processed: 110500/190636\n",
      "[ItemCosine-BM25@10] processed: 111000/190636\n",
      "[ItemCosine-BM25@10] processed: 111500/190636\n",
      "[ItemCosine-BM25@10] processed: 112000/190636\n",
      "[ItemCosine-BM25@10] processed: 112500/190636\n",
      "[ItemCosine-BM25@10] processed: 113000/190636\n",
      "[ItemCosine-BM25@10] processed: 113500/190636\n",
      "[ItemCosine-BM25@10] processed: 114000/190636\n",
      "[ItemCosine-BM25@10] processed: 114500/190636\n",
      "[ItemCosine-BM25@10] processed: 115000/190636\n",
      "[ItemCosine-BM25@10] processed: 115500/190636\n",
      "[ItemCosine-BM25@10] processed: 116000/190636\n",
      "[ItemCosine-BM25@10] processed: 116500/190636\n",
      "[ItemCosine-BM25@10] processed: 117000/190636\n",
      "[ItemCosine-BM25@10] processed: 117500/190636\n",
      "[ItemCosine-BM25@10] processed: 118000/190636\n",
      "[ItemCosine-BM25@10] processed: 118500/190636\n",
      "[ItemCosine-BM25@10] processed: 119000/190636\n",
      "[ItemCosine-BM25@10] processed: 119500/190636\n",
      "[ItemCosine-BM25@10] processed: 120000/190636\n",
      "[ItemCosine-BM25@10] processed: 120500/190636\n",
      "[ItemCosine-BM25@10] processed: 121000/190636\n",
      "[ItemCosine-BM25@10] processed: 121500/190636\n",
      "[ItemCosine-BM25@10] processed: 122000/190636\n",
      "[ItemCosine-BM25@10] processed: 122500/190636\n",
      "[ItemCosine-BM25@10] processed: 123000/190636\n",
      "[ItemCosine-BM25@10] processed: 123500/190636\n",
      "[ItemCosine-BM25@10] processed: 124000/190636\n",
      "[ItemCosine-BM25@10] processed: 124500/190636\n",
      "[ItemCosine-BM25@10] processed: 125000/190636\n",
      "[ItemCosine-BM25@10] processed: 125500/190636\n",
      "[ItemCosine-BM25@10] processed: 126000/190636\n",
      "[ItemCosine-BM25@10] processed: 126500/190636\n",
      "[ItemCosine-BM25@10] processed: 127000/190636\n",
      "[ItemCosine-BM25@10] processed: 127500/190636\n",
      "[ItemCosine-BM25@10] processed: 128000/190636\n",
      "[ItemCosine-BM25@10] processed: 128500/190636\n",
      "[ItemCosine-BM25@10] processed: 129000/190636\n",
      "[ItemCosine-BM25@10] processed: 129500/190636\n",
      "[ItemCosine-BM25@10] processed: 130000/190636\n",
      "[ItemCosine-BM25@10] processed: 130500/190636\n",
      "[ItemCosine-BM25@10] processed: 131000/190636\n",
      "[ItemCosine-BM25@10] processed: 131500/190636\n",
      "[ItemCosine-BM25@10] processed: 132000/190636\n",
      "[ItemCosine-BM25@10] processed: 132500/190636\n",
      "[ItemCosine-BM25@10] processed: 133000/190636\n",
      "[ItemCosine-BM25@10] processed: 133500/190636\n",
      "[ItemCosine-BM25@10] processed: 134000/190636\n",
      "[ItemCosine-BM25@10] processed: 134500/190636\n",
      "[ItemCosine-BM25@10] processed: 135000/190636\n",
      "[ItemCosine-BM25@10] processed: 135500/190636\n",
      "[ItemCosine-BM25@10] processed: 136000/190636\n",
      "[ItemCosine-BM25@10] processed: 136500/190636\n",
      "[ItemCosine-BM25@10] processed: 137000/190636\n",
      "[ItemCosine-BM25@10] processed: 137500/190636\n",
      "[ItemCosine-BM25@10] processed: 138000/190636\n",
      "[ItemCosine-BM25@10] processed: 138500/190636\n",
      "[ItemCosine-BM25@10] processed: 139000/190636\n",
      "[ItemCosine-BM25@10] processed: 139500/190636\n",
      "[ItemCosine-BM25@10] processed: 140000/190636\n",
      "[ItemCosine-BM25@10] processed: 140500/190636\n",
      "[ItemCosine-BM25@10] processed: 141000/190636\n",
      "[ItemCosine-BM25@10] processed: 141500/190636\n",
      "[ItemCosine-BM25@10] processed: 142000/190636\n",
      "[ItemCosine-BM25@10] processed: 142500/190636\n",
      "[ItemCosine-BM25@10] processed: 143000/190636\n",
      "[ItemCosine-BM25@10] processed: 143500/190636\n",
      "[ItemCosine-BM25@10] processed: 144000/190636\n",
      "[ItemCosine-BM25@10] processed: 144500/190636\n",
      "[ItemCosine-BM25@10] processed: 145000/190636\n",
      "[ItemCosine-BM25@10] processed: 145500/190636\n",
      "[ItemCosine-BM25@10] processed: 146000/190636\n",
      "[ItemCosine-BM25@10] processed: 146500/190636\n",
      "[ItemCosine-BM25@10] processed: 147000/190636\n",
      "[ItemCosine-BM25@10] processed: 147500/190636\n",
      "[ItemCosine-BM25@10] processed: 148000/190636\n",
      "[ItemCosine-BM25@10] processed: 148500/190636\n",
      "[ItemCosine-BM25@10] processed: 149000/190636\n",
      "[ItemCosine-BM25@10] processed: 149500/190636\n",
      "[ItemCosine-BM25@10] processed: 150000/190636\n",
      "[ItemCosine-BM25@10] processed: 150500/190636\n",
      "[ItemCosine-BM25@10] processed: 151000/190636\n",
      "[ItemCosine-BM25@10] processed: 151500/190636\n",
      "[ItemCosine-BM25@10] processed: 152000/190636\n",
      "[ItemCosine-BM25@10] processed: 152500/190636\n",
      "[ItemCosine-BM25@10] processed: 153000/190636\n",
      "[ItemCosine-BM25@10] processed: 153500/190636\n",
      "[ItemCosine-BM25@10] processed: 154000/190636\n",
      "[ItemCosine-BM25@10] processed: 154500/190636\n",
      "[ItemCosine-BM25@10] processed: 155000/190636\n",
      "[ItemCosine-BM25@10] processed: 155500/190636\n",
      "[ItemCosine-BM25@10] processed: 156000/190636\n",
      "[ItemCosine-BM25@10] processed: 156500/190636\n",
      "[ItemCosine-BM25@10] processed: 157000/190636\n",
      "[ItemCosine-BM25@10] processed: 157500/190636\n",
      "[ItemCosine-BM25@10] processed: 158000/190636\n",
      "[ItemCosine-BM25@10] processed: 158500/190636\n",
      "[ItemCosine-BM25@10] processed: 159000/190636\n",
      "[ItemCosine-BM25@10] processed: 159500/190636\n",
      "[ItemCosine-BM25@10] processed: 160000/190636\n",
      "[ItemCosine-BM25@10] processed: 160500/190636\n",
      "[ItemCosine-BM25@10] processed: 161000/190636\n",
      "[ItemCosine-BM25@10] processed: 161500/190636\n",
      "[ItemCosine-BM25@10] processed: 162000/190636\n",
      "[ItemCosine-BM25@10] processed: 162500/190636\n",
      "[ItemCosine-BM25@10] processed: 163000/190636\n",
      "[ItemCosine-BM25@10] processed: 163500/190636\n",
      "[ItemCosine-BM25@10] processed: 164000/190636\n",
      "[ItemCosine-BM25@10] processed: 164500/190636\n",
      "[ItemCosine-BM25@10] processed: 165000/190636\n",
      "[ItemCosine-BM25@10] processed: 165500/190636\n",
      "[ItemCosine-BM25@10] processed: 166000/190636\n",
      "[ItemCosine-BM25@10] processed: 166500/190636\n",
      "[ItemCosine-BM25@10] processed: 167000/190636\n",
      "[ItemCosine-BM25@10] processed: 167500/190636\n",
      "[ItemCosine-BM25@10] processed: 168000/190636\n",
      "[ItemCosine-BM25@10] processed: 168500/190636\n",
      "[ItemCosine-BM25@10] processed: 169000/190636\n",
      "[ItemCosine-BM25@10] processed: 169500/190636\n",
      "[ItemCosine-BM25@10] processed: 170000/190636\n",
      "[ItemCosine-BM25@10] processed: 170500/190636\n",
      "[ItemCosine-BM25@10] processed: 171000/190636\n",
      "[ItemCosine-BM25@10] processed: 171500/190636\n",
      "[ItemCosine-BM25@10] processed: 172000/190636\n",
      "[ItemCosine-BM25@10] processed: 172500/190636\n",
      "[ItemCosine-BM25@10] processed: 173000/190636\n",
      "[ItemCosine-BM25@10] processed: 173500/190636\n",
      "[ItemCosine-BM25@10] processed: 174000/190636\n",
      "[ItemCosine-BM25@10] processed: 174500/190636\n",
      "[ItemCosine-BM25@10] processed: 175000/190636\n",
      "[ItemCosine-BM25@10] processed: 175500/190636\n",
      "[ItemCosine-BM25@10] processed: 176000/190636\n",
      "[ItemCosine-BM25@10] processed: 176500/190636\n",
      "[ItemCosine-BM25@10] processed: 177000/190636\n",
      "[ItemCosine-BM25@10] processed: 177500/190636\n",
      "[ItemCosine-BM25@10] processed: 178000/190636\n",
      "[ItemCosine-BM25@10] processed: 178500/190636\n",
      "[ItemCosine-BM25@10] processed: 179000/190636\n",
      "[ItemCosine-BM25@10] processed: 179500/190636\n",
      "[ItemCosine-BM25@10] processed: 180000/190636\n",
      "[ItemCosine-BM25@10] processed: 180500/190636\n",
      "[ItemCosine-BM25@10] processed: 181000/190636\n",
      "[ItemCosine-BM25@10] processed: 181500/190636\n",
      "[ItemCosine-BM25@10] processed: 182000/190636\n",
      "[ItemCosine-BM25@10] processed: 182500/190636\n",
      "[ItemCosine-BM25@10] processed: 183000/190636\n",
      "[ItemCosine-BM25@10] processed: 183500/190636\n",
      "[ItemCosine-BM25@10] processed: 184000/190636\n",
      "[ItemCosine-BM25@10] processed: 184500/190636\n",
      "[ItemCosine-BM25@10] processed: 185000/190636\n",
      "[ItemCosine-BM25@10] processed: 185500/190636\n",
      "[ItemCosine-BM25@10] processed: 186000/190636\n",
      "[ItemCosine-BM25@10] processed: 186500/190636\n",
      "[ItemCosine-BM25@10] processed: 187000/190636\n",
      "[ItemCosine-BM25@10] processed: 187500/190636\n",
      "[ItemCosine-BM25@10] processed: 188000/190636\n",
      "[ItemCosine-BM25@10] processed: 188500/190636\n",
      "[ItemCosine-BM25@10] processed: 189000/190636\n",
      "[ItemCosine-BM25@10] processed: 189500/190636\n",
      "[ItemCosine-BM25@10] processed: 190000/190636\n",
      "[ItemCosine-BM25@10] processed: 190500/190636\n",
      "[ItemCosine-BM25@10] processed: 190636/190636\n",
      "[SVD-BM25@10] processed: 1500/190636\n",
      "[SVD-BM25@10] processed: 3000/190636\n",
      "[SVD-BM25@10] processed: 4500/190636\n",
      "[SVD-BM25@10] processed: 6000/190636\n",
      "[SVD-BM25@10] processed: 7500/190636\n",
      "[SVD-BM25@10] processed: 9000/190636\n",
      "[SVD-BM25@10] processed: 10500/190636\n",
      "[SVD-BM25@10] processed: 12000/190636\n",
      "[SVD-BM25@10] processed: 13500/190636\n",
      "[SVD-BM25@10] processed: 15000/190636\n",
      "[SVD-BM25@10] processed: 16500/190636\n",
      "[SVD-BM25@10] processed: 18000/190636\n",
      "[SVD-BM25@10] processed: 19500/190636\n",
      "[SVD-BM25@10] processed: 21000/190636\n",
      "[SVD-BM25@10] processed: 22500/190636\n",
      "[SVD-BM25@10] processed: 24000/190636\n",
      "[SVD-BM25@10] processed: 25500/190636\n",
      "[SVD-BM25@10] processed: 27000/190636\n",
      "[SVD-BM25@10] processed: 28500/190636\n",
      "[SVD-BM25@10] processed: 30000/190636\n",
      "[SVD-BM25@10] processed: 31500/190636\n",
      "[SVD-BM25@10] processed: 33000/190636\n",
      "[SVD-BM25@10] processed: 34500/190636\n",
      "[SVD-BM25@10] processed: 36000/190636\n",
      "[SVD-BM25@10] processed: 37500/190636\n",
      "[SVD-BM25@10] processed: 39000/190636\n",
      "[SVD-BM25@10] processed: 40500/190636\n",
      "[SVD-BM25@10] processed: 42000/190636\n",
      "[SVD-BM25@10] processed: 43500/190636\n",
      "[SVD-BM25@10] processed: 45000/190636\n",
      "[SVD-BM25@10] processed: 46500/190636\n",
      "[SVD-BM25@10] processed: 48000/190636\n",
      "[SVD-BM25@10] processed: 49500/190636\n",
      "[SVD-BM25@10] processed: 51000/190636\n",
      "[SVD-BM25@10] processed: 52500/190636\n",
      "[SVD-BM25@10] processed: 54000/190636\n",
      "[SVD-BM25@10] processed: 55500/190636\n",
      "[SVD-BM25@10] processed: 57000/190636\n",
      "[SVD-BM25@10] processed: 58500/190636\n",
      "[SVD-BM25@10] processed: 60000/190636\n",
      "[SVD-BM25@10] processed: 61500/190636\n",
      "[SVD-BM25@10] processed: 63000/190636\n",
      "[SVD-BM25@10] processed: 64500/190636\n",
      "[SVD-BM25@10] processed: 66000/190636\n",
      "[SVD-BM25@10] processed: 67500/190636\n",
      "[SVD-BM25@10] processed: 69000/190636\n",
      "[SVD-BM25@10] processed: 70500/190636\n",
      "[SVD-BM25@10] processed: 72000/190636\n",
      "[SVD-BM25@10] processed: 73500/190636\n",
      "[SVD-BM25@10] processed: 75000/190636\n",
      "[SVD-BM25@10] processed: 76500/190636\n",
      "[SVD-BM25@10] processed: 78000/190636\n",
      "[SVD-BM25@10] processed: 79500/190636\n",
      "[SVD-BM25@10] processed: 81000/190636\n",
      "[SVD-BM25@10] processed: 82500/190636\n",
      "[SVD-BM25@10] processed: 84000/190636\n",
      "[SVD-BM25@10] processed: 85500/190636\n",
      "[SVD-BM25@10] processed: 87000/190636\n",
      "[SVD-BM25@10] processed: 88500/190636\n",
      "[SVD-BM25@10] processed: 90000/190636\n",
      "[SVD-BM25@10] processed: 91500/190636\n",
      "[SVD-BM25@10] processed: 93000/190636\n",
      "[SVD-BM25@10] processed: 94500/190636\n",
      "[SVD-BM25@10] processed: 96000/190636\n",
      "[SVD-BM25@10] processed: 97500/190636\n",
      "[SVD-BM25@10] processed: 99000/190636\n",
      "[SVD-BM25@10] processed: 100500/190636\n",
      "[SVD-BM25@10] processed: 102000/190636\n",
      "[SVD-BM25@10] processed: 103500/190636\n",
      "[SVD-BM25@10] processed: 105000/190636\n",
      "[SVD-BM25@10] processed: 106500/190636\n",
      "[SVD-BM25@10] processed: 108000/190636\n",
      "[SVD-BM25@10] processed: 109500/190636\n",
      "[SVD-BM25@10] processed: 111000/190636\n",
      "[SVD-BM25@10] processed: 112500/190636\n",
      "[SVD-BM25@10] processed: 114000/190636\n",
      "[SVD-BM25@10] processed: 115500/190636\n",
      "[SVD-BM25@10] processed: 117000/190636\n",
      "[SVD-BM25@10] processed: 118500/190636\n",
      "[SVD-BM25@10] processed: 120000/190636\n",
      "[SVD-BM25@10] processed: 121500/190636\n",
      "[SVD-BM25@10] processed: 123000/190636\n",
      "[SVD-BM25@10] processed: 124500/190636\n",
      "[SVD-BM25@10] processed: 126000/190636\n",
      "[SVD-BM25@10] processed: 127500/190636\n",
      "[SVD-BM25@10] processed: 129000/190636\n",
      "[SVD-BM25@10] processed: 130500/190636\n",
      "[SVD-BM25@10] processed: 132000/190636\n",
      "[SVD-BM25@10] processed: 133500/190636\n",
      "[SVD-BM25@10] processed: 135000/190636\n",
      "[SVD-BM25@10] processed: 136500/190636\n",
      "[SVD-BM25@10] processed: 138000/190636\n",
      "[SVD-BM25@10] processed: 139500/190636\n",
      "[SVD-BM25@10] processed: 141000/190636\n",
      "[SVD-BM25@10] processed: 142500/190636\n",
      "[SVD-BM25@10] processed: 144000/190636\n",
      "[SVD-BM25@10] processed: 145500/190636\n",
      "[SVD-BM25@10] processed: 147000/190636\n",
      "[SVD-BM25@10] processed: 148500/190636\n",
      "[SVD-BM25@10] processed: 150000/190636\n",
      "[SVD-BM25@10] processed: 151500/190636\n",
      "[SVD-BM25@10] processed: 153000/190636\n",
      "[SVD-BM25@10] processed: 154500/190636\n",
      "[SVD-BM25@10] processed: 156000/190636\n",
      "[SVD-BM25@10] processed: 157500/190636\n",
      "[SVD-BM25@10] processed: 159000/190636\n",
      "[SVD-BM25@10] processed: 160500/190636\n",
      "[SVD-BM25@10] processed: 162000/190636\n",
      "[SVD-BM25@10] processed: 163500/190636\n",
      "[SVD-BM25@10] processed: 165000/190636\n",
      "[SVD-BM25@10] processed: 166500/190636\n",
      "[SVD-BM25@10] processed: 168000/190636\n",
      "[SVD-BM25@10] processed: 169500/190636\n",
      "[SVD-BM25@10] processed: 171000/190636\n",
      "[SVD-BM25@10] processed: 172500/190636\n",
      "[SVD-BM25@10] processed: 174000/190636\n",
      "[SVD-BM25@10] processed: 175500/190636\n",
      "[SVD-BM25@10] processed: 177000/190636\n",
      "[SVD-BM25@10] processed: 178500/190636\n",
      "[SVD-BM25@10] processed: 180000/190636\n",
      "[SVD-BM25@10] processed: 181500/190636\n",
      "[SVD-BM25@10] processed: 183000/190636\n",
      "[SVD-BM25@10] processed: 184500/190636\n",
      "[SVD-BM25@10] processed: 186000/190636\n",
      "[SVD-BM25@10] processed: 187500/190636\n",
      "[SVD-BM25@10] processed: 189000/190636\n",
      "[SVD-BM25@10] processed: 190500/190636\n",
      "[SVD-BM25@10] processed: 190636/190636\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>users</th>\n",
       "      <th>recall</th>\n",
       "      <th>map</th>\n",
       "      <th>ndcg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Popularity@10</td>\n",
       "      <td>190636</td>\n",
       "      <td>0.001673</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>0.000742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ItemCosine-RAW@10</td>\n",
       "      <td>190636</td>\n",
       "      <td>0.035229</td>\n",
       "      <td>0.015882</td>\n",
       "      <td>0.020419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ItemCosine-TFIDF@10</td>\n",
       "      <td>190636</td>\n",
       "      <td>0.035476</td>\n",
       "      <td>0.015988</td>\n",
       "      <td>0.020558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ItemCosine-BM25@10</td>\n",
       "      <td>190636</td>\n",
       "      <td>0.077498</td>\n",
       "      <td>0.030939</td>\n",
       "      <td>0.041785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVD-BM25@10</td>\n",
       "      <td>190636</td>\n",
       "      <td>0.008262</td>\n",
       "      <td>0.003269</td>\n",
       "      <td>0.004429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 label   users    recall       map      ndcg\n",
       "0        Popularity@10  190636  0.001673  0.000469  0.000742\n",
       "1    ItemCosine-RAW@10  190636  0.035229  0.015882  0.020419\n",
       "2  ItemCosine-TFIDF@10  190636  0.035476  0.015988  0.020558\n",
       "3   ItemCosine-BM25@10  190636  0.077498  0.030939  0.041785\n",
       "4          SVD-BM25@10  190636  0.008262  0.003269  0.004429"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -------- helpers: masking & topk extraction (batched) --------\n",
    "def _mask_seen_rows(scores, batch_user_idx, seen_train, neg_val=-np.inf):\n",
    "    \"\"\"Set scores of seen items to -inf for each user row.\"\"\"\n",
    "    for r, u_idx in enumerate(batch_user_idx):\n",
    "        s = seen_train.get(u_idx, None)\n",
    "        if s:\n",
    "            scores[r, list(s)] = neg_val\n",
    "\n",
    "def _rowwise_topk_indices(scores, k):\n",
    "    \"\"\"\n",
    "    For a 2D array (n_rows x n_items), return per-row top-k item indices (by column index).\n",
    "    Uses argpartition -> arg sort on the k-slice for speed.\n",
    "    \"\"\"\n",
    "    # argpartition gives unordered top-k positions\n",
    "    part = np.argpartition(-scores, kth=k-1, axis=1)[:, :k]\n",
    "    # order those k positions properly per row\n",
    "    row_idx = np.arange(scores.shape[0])[:, None]\n",
    "    part_scores = scores[row_idx, part]\n",
    "    order = np.argsort(-part_scores, axis=1)\n",
    "    return part[row_idx, order]\n",
    "\n",
    "# -------- batched recommenders (no per-user Python loops) --------\n",
    "def batch_toplist_pop(batch_user_idx, pop_order, k):\n",
    "    \"\"\"\n",
    "    Popularity baseline; same list for everyone, but we filter seen items user-wise.\n",
    "    We’ll opportunistically scan a bit more than k to avoid many re-scans.\n",
    "    \"\"\"\n",
    "    k_probe = min(len(pop_order), k + 200)  # small headroom to compensate for seen filtering\n",
    "    head = pop_order[:k_probe]\n",
    "    out = []\n",
    "    for u_idx in batch_user_idx:\n",
    "        seen = seen_train.get(u_idx, set())\n",
    "        rec = [idx2item[j] for j in head if j not in seen]\n",
    "        if len(rec) < k:\n",
    "            # fallback: extend scan if needed\n",
    "            for j in pop_order[k_probe:]:\n",
    "                if j not in seen:\n",
    "                    rec.append(idx2item[j])\n",
    "                    if len(rec) == k:\n",
    "                        break\n",
    "        out.append(rec[:k])\n",
    "    return out\n",
    "\n",
    "def batch_toplist_itemcosine(batch_user_idx, X_base, C, k):\n",
    "    \"\"\"\n",
    "    Item–item cosine in batch:\n",
    "      scores = (W @ C^T) @ C, where W = X_base[batch, :]\n",
    "    \"\"\"\n",
    "    W = X_base[batch_user_idx, :]               # (B x n_items) sparse\n",
    "    Y = (W @ C.T)                               # (B x n_users) sparse\n",
    "    S = (Y @ C).toarray()                       # (B x n_items) dense\n",
    "    _mask_seen_rows(S, batch_user_idx, seen_train, neg_val=-np.inf)\n",
    "    topk_cols = _rowwise_topk_indices(S, k)     # (B x k) item indices (train space)\n",
    "    return [[idx2item[j] for j in row] for row in topk_cols]\n",
    "\n",
    "def batch_toplist_svd(batch_user_idx, P, Q, k):\n",
    "    \"\"\"\n",
    "    SVD scoring: scores = P_batch @ Q^T (dense x dense)\n",
    "    \"\"\"\n",
    "    P_batch = P[batch_user_idx, :]              # (B x f)\n",
    "    S = P_batch @ Q.T                           # (B x n_items)\n",
    "    _mask_seen_rows(S, batch_user_idx, seen_train, neg_val=-np.inf)\n",
    "    topk_cols = _rowwise_topk_indices(S, k)\n",
    "    return [[idx2item[j] for j in row] for row in topk_cols]\n",
    "\n",
    "# -------- metrics (same as before) --------\n",
    "def recall_at_k(true_items, rec_list, k=10):\n",
    "    if not true_items: return 0.0\n",
    "    return len(set(rec_list[:k]) & set(true_items)) / len(true_items)\n",
    "\n",
    "def ap_at_k(true_items, rec_list, k=10):\n",
    "    if not true_items: return 0.0\n",
    "    true = set(true_items)\n",
    "    hits, ap = 0, 0.0\n",
    "    for i, it in enumerate(rec_list[:k], start=1):\n",
    "        if it in true:\n",
    "            hits += 1\n",
    "            ap += hits / i\n",
    "    return ap / min(len(true_items), k)\n",
    "\n",
    "def ndcg_at_k(true_items, rec_list, k=10):\n",
    "    if not true_items: return 0.0\n",
    "    true = set(true_items)\n",
    "    dcg = 0.0\n",
    "    for i, it in enumerate(rec_list[:k], start=1):\n",
    "        if it in true:\n",
    "            dcg += 1.0 / np.log2(i+1)\n",
    "    ideal = min(len(true_items), k)\n",
    "    idcg = sum(1.0 / np.log2(i+1) for i in range(1, ideal+1))\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "# -------- batched evaluator (all users, but processed in chunks) --------\n",
    "def eval_batched(model_name, batch_func, k=K, batch_size=20000, verbose=True):\n",
    "    users_eval = np.array([u for u in eval_users if u in user2idx])\n",
    "    n = len(users_eval)\n",
    "    recalls, maps, ndcgs = [], [], []\n",
    "    done = 0\n",
    "\n",
    "    for start in range(0, n, batch_size):\n",
    "        batch_users_orig = users_eval[start:start+batch_size]\n",
    "        batch_idx = np.array([user2idx[u] for u in batch_users_orig], dtype=int)\n",
    "\n",
    "        # truth per user (in original item ids)\n",
    "        truth_list = []\n",
    "        keep_mask = []\n",
    "        for u_orig in batch_users_orig:\n",
    "            t = [it for it in test_truth[u_orig] if it in item2idx]  # keep items known to train\n",
    "            truth_list.append(t)\n",
    "            keep_mask.append(len(t) > 0)\n",
    "\n",
    "        # filter out users with empty truth in this batch (saves scoring time)\n",
    "        keep_mask = np.array(keep_mask, dtype=bool)\n",
    "        if not keep_mask.any():\n",
    "            done += len(batch_users_orig)\n",
    "            if verbose: print(f\"[{model_name}] {done}/{n} users (no truth in batch)\")\n",
    "            continue\n",
    "        batch_users_orig = batch_users_orig[keep_mask]\n",
    "        batch_idx = batch_idx[keep_mask]\n",
    "        truth_list = [truth_list[i] for i in np.where(keep_mask)[0]]\n",
    "\n",
    "        # get recommendations for this batch (list of lists of itemids)\n",
    "        recs_batch = batch_func(batch_idx, k)\n",
    "\n",
    "        # compute metrics\n",
    "        for truth, rec in zip(truth_list, recs_batch):\n",
    "            recalls.append(recall_at_k(truth, rec, k))\n",
    "            maps.append(ap_at_k(truth, rec, k))\n",
    "            ndcgs.append(ndcg_at_k(truth, rec, k))\n",
    "\n",
    "        done += len(keep_mask)\n",
    "        if verbose:\n",
    "            print(f\"[{model_name}] processed: {min(done,n)}/{n}\")\n",
    "\n",
    "    return {\n",
    "        \"label\": model_name,\n",
    "        \"users\": len(recalls),\n",
    "        \"recall\": np.mean(recalls) if recalls else 0.0,\n",
    "        \"map\":    np.mean(maps)    if maps    else 0.0,\n",
    "        \"ndcg\":   np.mean(ndcgs)   if ndcgs   else 0.0,\n",
    "    }\n",
    "\n",
    "# -------- run models (batched) --------\n",
    "rows = []\n",
    "\n",
    "# Popularity (precomputed order from earlier cell)\n",
    "rows.append(eval_batched(\n",
    "    \"Popularity@10\",\n",
    "    batch_func=lambda batch_idx, k=K: batch_toplist_pop(batch_idx, pop_order, k),\n",
    "    k=K, batch_size=2000, verbose=True\n",
    "))\n",
    "\n",
    "# Item–Item cosine (RAW / TF-IDF / BM25)\n",
    "rows.append(eval_batched(\n",
    "    \"ItemCosine-RAW@10\",\n",
    "    batch_func=lambda batch_idx, k=K: batch_toplist_itemcosine(batch_idx, X_RAW,   C_RAW,   k),\n",
    "    k=K, batch_size=500, verbose=True   # item-cosine is heavier; smaller batch helps memory\n",
    "))\n",
    "rows.append(eval_batched(\n",
    "    \"ItemCosine-TFIDF@10\",\n",
    "    batch_func=lambda batch_idx, k=K: batch_toplist_itemcosine(batch_idx, X_TFIDF, C_TFIDF, k),\n",
    "    k=K, batch_size=500, verbose=True\n",
    "))\n",
    "rows.append(eval_batched(\n",
    "    \"ItemCosine-BM25@10\",\n",
    "    batch_func=lambda batch_idx, k=K: batch_toplist_itemcosine(batch_idx, X_BM25,  C_BM25,  k),\n",
    "    k=K, batch_size=500, verbose=True\n",
    "))\n",
    "\n",
    "# SVD on BM25\n",
    "rows.append(eval_batched(\n",
    "    \"SVD-BM25@10\",\n",
    "    batch_func=lambda batch_idx, k=K: batch_toplist_svd(batch_idx, P_bm25, Q_bm25, k),\n",
    "    k=K, batch_size=1500, verbose=True\n",
    "))\n",
    "\n",
    "eval_df = pd.DataFrame(rows)\n",
    "eval_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c7927b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(9,5))\n",
    "x = np.arange(len(eval_df))\n",
    "w = 0.25\n",
    "ax.bar(x - w, eval_df['recall'],   width=w, label='Recall@10')\n",
    "ax.bar(x,       eval_df['map'],     width=w, label='MAP@10')\n",
    "ax.bar(x + w,   eval_df['ndcg'],    width=w, label='NDCG@10')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(eval_df['label'], rotation=20, ha='right')\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "for spine in ('top','right'): ax.spines[spine].set_visible(False)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdd3e9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "from scipy.sparse import coo_matrix, csr_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# ========= KNOBS (tune for speed vs quality) ========= #\n",
    "K = 10                       # top-K for evaluation\n",
    "EVENT_WEIGHTS = {'view': 1.0, 'addtocart': 5.0, 'transaction': 20.0}\n",
    "HALF_LIFE_DAYS = 21         # recency half-life for decay\n",
    "BM25_K1 = 1.2               # BM25 parameters\n",
    "BM25_B  = 0.75\n",
    "\n",
    "CAND_BM25 = 200             # #candidates from BM25 per user\n",
    "CAND_COVIS = 100            # #candidates from co-vis per user\n",
    "CAND_POP = 100              # #candidates from popularity per user\n",
    "MAX_CANDS_PER_USER = 400\n",
    "\n",
    "# co-vis session/window parameters\n",
    "SESSION_GAP_MIN = 30\n",
    "COVIS_WINDOW = 5            # consider +/- this many neighbors in a session\n",
    "TOP_COVIS_PER_ITEM = 50     # store top-N neighbors per item in co-vis table\n",
    "\n",
    "# evaluation chunking\n",
    "EVAL_CHUNK_USERS = 20_000\n",
    "\n",
    "# ===================================================== #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a939a33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure types\n",
    "df = pd.read_csv(\"../Data/Cleaned Dataset/final_merged_events.csv\")\n",
    "df = df.copy()\n",
    "df['event_time'] = pd.to_datetime(df['event_time'])\n",
    "df = df.sort_values('event_time')\n",
    "\n",
    "# Keep only events we care about\n",
    "df = df[df['event'].isin(['view', 'addtocart', 'transaction'])]\n",
    "\n",
    "# Add date for temporal splits & recency\n",
    "df['date'] = df['event_time'].dt.date\n",
    "df['ts'] = df['event_time'].astype('int64') // 10**9  # seconds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6c0ba0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 2015-05-03 03:00:04.384000 → 2015-08-05 18:04:34.556000 1984060\n",
      "Val  : 2015-08-05 18:04:38.117000 → 2015-08-18 04:23:01.129000 220452\n",
      "Test : 2015-08-18 04:23:20.445000 → 2015-09-18 02:59:47.788000 551129\n"
     ]
    }
   ],
   "source": [
    "times = df['event_time'].sort_values()\n",
    "t80 = times.quantile(0.8)\n",
    "t90_train = times[times <= t80].quantile(0.9)\n",
    "\n",
    "df_train_full = df[df['event_time'] <= t80]\n",
    "df_val = df_train_full[df_train_full['event_time'] > t90_train]\n",
    "df_train = df_train_full[df_train_full['event_time'] <= t90_train]\n",
    "df_test  = df[df['event_time'] > t80]\n",
    "\n",
    "print(\"Train:\", df_train['event_time'].min(), \"→\", df_train['event_time'].max(), len(df_train))\n",
    "print(\"Val  :\", df_val['event_time'].min(),   \"→\", df_val['event_time'].max(),   len(df_val))\n",
    "print(\"Test :\", df_test['event_time'].min(),  \"→\", df_test['event_time'].max(),  len(df_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4749ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recency decay weights\n",
    "def recency_decay(ts_series, ref_ts, half_life_days=HALF_LIFE_DAYS):\n",
    "    hl_seconds = half_life_days * 24 * 3600\n",
    "    dt = (ref_ts - ts_series).clip(lower=0)\n",
    "    return np.exp(-np.log(2) * dt / hl_seconds)\n",
    "\n",
    "# reference time = end of train\n",
    "ref_ts = int(df_train['ts'].max())\n",
    "\n",
    "def make_weighted_interactions(df_part):\n",
    "    base = df_part['event'].map(EVENT_WEIGHTS).astype(float)\n",
    "    decay = recency_decay(df_part['ts'], ref_ts)\n",
    "    return base * decay\n",
    "\n",
    "df_train = df_train.copy()\n",
    "df_train['w'] = make_weighted_interactions(df_train)\n",
    "\n",
    "# Build user & item index maps from TRAIN ONLY\n",
    "unique_users = df_train['visitorid'].unique()\n",
    "unique_items = df_train['itemid'].unique()\n",
    "user2idx = {u:i for i,u in enumerate(unique_users)}\n",
    "item2idx = {it:j for j,it in enumerate(unique_items)}\n",
    "\n",
    "# Sparse matrix (users x items) from train\n",
    "rows = df_train['visitorid'].map(user2idx).values\n",
    "cols = df_train['itemid'].map(item2idx).values\n",
    "data = df_train['w'].values\n",
    "X_train_raw = coo_matrix((data, (rows, cols)),\n",
    "                         shape=(len(unique_users), len(unique_items))).tocsr()\n",
    "\n",
    "# BM25 weighting on users x items\n",
    "def bm25_weight(X, K1=BM25_K1, B=BM25_B):\n",
    "    # X is csr (users x items)\n",
    "    # BM25 per item (column) using per-user lengths\n",
    "    X = X.tocoo()\n",
    "    n_users, n_items = X.shape\n",
    "    # doc length = per-user nnz sum\n",
    "    user_len = np.asarray(X.sum(axis=1)).ravel()\n",
    "    avg_len = user_len.mean() if n_users > 0 else 1.0\n",
    "\n",
    "    # idf per item\n",
    "    # df_j = number of users who interacted with item j\n",
    "    df_items = np.diff(X.tocsc().indptr)  # length = n_items\n",
    "    idf = np.log((n_users - df_items + 0.5) / (df_items + 0.5)).clip(min=0)\n",
    "\n",
    "    # BM25 transform each non-zero x_ij\n",
    "    # w_ij = idf_j * (x_ij*(K1+1)) / (x_ij + K1*(1 - B + B*len_i/avg_len))\n",
    "    x = X.data\n",
    "    i = X.row\n",
    "    j = X.col\n",
    "    denom = x + K1 * (1.0 - B + B * (user_len[i] / (avg_len + 1e-9)))\n",
    "    w = idf[j] * (x * (K1 + 1.0)) / (denom + 1e-9)\n",
    "    X_bm25 = coo_matrix((w, (i, j)), shape=X.shape).tocsr()\n",
    "    return X_bm25\n",
    "\n",
    "X_train_bm25 = bm25_weight(X_train_raw)\n",
    "\n",
    "# L2-normalize item columns for cosine scoring\n",
    "def l2_normalize_columns(X):\n",
    "    Xc = X.tocsc(copy=True)\n",
    "    norms = np.sqrt(np.asarray(Xc.power(2).sum(axis=0)).ravel()) + 1e-12\n",
    "    Xc.data /= norms[Xc.indices]  # careful: indices on CSC are row indices\n",
    "    return Xc.tocsr()\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "X_cols_norm = normalize(X_train_bm25, norm='l2', axis=0, copy=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c052afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Popularity (weighted) from train\n",
    "pop_item_counts = (\n",
    "    df_train.groupby('itemid')['w'].sum().sort_values(ascending=False)\n",
    ")\n",
    "popular_items_idx = [item2idx[it] for it in pop_item_counts.index if it in item2idx]\n",
    "\n",
    "# ---- Co-visitation (session-level) ----\n",
    "def build_covis(df_part, session_gap_min=SESSION_GAP_MIN, window=COVIS_WINDOW, topN=TOP_COVIS_PER_ITEM):\n",
    "    # work on views+addtocart+transaction (optionally views only)\n",
    "    d = df_part[['visitorid','itemid','event_time']].copy()\n",
    "    d = d.sort_values(['visitorid','event_time'])\n",
    "    # sessionize per user\n",
    "    d['prev_time'] = d.groupby('visitorid')['event_time'].shift(1)\n",
    "    gap = (d['event_time'] - d['prev_time']).dt.total_seconds().fillna(0)\n",
    "    d['session_id'] = (gap > session_gap_min*60).groupby(d['visitorid']).cumsum()\n",
    "\n",
    "    covis = defaultdict(Counter)\n",
    "    for (uid, sid), g in d.groupby(['visitorid','session_id'], sort=False):\n",
    "        items = g['itemid'].tolist()\n",
    "        n = len(items)\n",
    "        # sliding window pairs\n",
    "        for p in range(n):\n",
    "            a = items[p]\n",
    "            start = max(0, p - window)\n",
    "            end   = min(n, p + window + 1)\n",
    "            for q in range(start, end):\n",
    "                if q == p: \n",
    "                    continue\n",
    "                b = items[q]\n",
    "                covis[a][b] += 1\n",
    "    # keep top neighbors\n",
    "    covis_top = {}\n",
    "    for a, ctr in covis.items():\n",
    "        covis_top[a] = [b for b,_ in ctr.most_common(topN)]\n",
    "    return covis_top\n",
    "\n",
    "covis_neighbors = build_covis(df_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e1665ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: get seen items per user (from train)\n",
    "seen_by_user = defaultdict(set)\n",
    "for u, it in zip(df_train['visitorid'].map(user2idx, na_action='ignore'),\n",
    "                 df_train['itemid'].map(item2idx, na_action='ignore')):\n",
    "    if pd.isna(u) or pd.isna(it): \n",
    "        continue\n",
    "    seen_by_user[int(u)].add(int(it))\n",
    "\n",
    "def bm25_candidates_for_user(u_idx, topn=CAND_BM25):\n",
    "    # user profile row (1 x n_items)\n",
    "    u_row = X_train_bm25[u_idx, :]\n",
    "    scores = u_row.dot(X_cols_norm).toarray().ravel()\n",
    "    # remove seen\n",
    "    if seen_by_user.get(u_idx):\n",
    "        scores[list(seen_by_user[u_idx])] = -1e12\n",
    "    idx = np.argpartition(-scores, kth=min(topn, len(scores)-1))[:topn]\n",
    "    idx = idx[np.argsort(-scores[idx])]\n",
    "    return idx, scores[idx]\n",
    "\n",
    "def covis_candidates_for_user(u_idx, topn=CAND_COVIS):\n",
    "    # take most recent items for that user in train\n",
    "    uid = unique_users[u_idx]\n",
    "    last_items = (\n",
    "        df_train[df_train['visitorid']==uid]\n",
    "        .sort_values('event_time')\n",
    "        .tail(20)['itemid'].tolist()\n",
    "    )\n",
    "    cand = []\n",
    "    for it in reversed(last_items):\n",
    "        neighbors = covis_neighbors.get(it, [])\n",
    "        cand.extend(neighbors)\n",
    "        if len(cand) >= topn*2:  # oversample; will de-dup later\n",
    "            break\n",
    "    cand = [item2idx[it] for it in cand if it in item2idx]\n",
    "    # remove seen + keep topn by frequency approximation\n",
    "    cand = [c for c in cand if c not in seen_by_user.get(u_idx, set())]\n",
    "    # simple frequency sort\n",
    "    fre = Counter(cand)\n",
    "    top = [c for c,_ in fre.most_common(topn)]\n",
    "    # fake scores = frequency\n",
    "    sc = np.array([fre[c] for c in top], dtype=float)\n",
    "    return np.array(top, dtype=int), sc\n",
    "\n",
    "def pop_candidates_for_user(u_idx, topn=CAND_POP):\n",
    "    # remove seen\n",
    "    seen = seen_by_user.get(u_idx, set())\n",
    "    out = [j for j in popular_items_idx if j not in seen][:topn]\n",
    "    sc  = np.arange(len(out), 0, -1, dtype=float)  # simple descending score\n",
    "    return np.array(out, dtype=int), sc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c70b9334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== FAST PRECOMPUTE OF SIGNALS (vectorized, memory-aware) ====\n",
    "# expects: df_train with columns ['visitorid','itemid','event','event_time','w','categoryid']\n",
    "#          unique_items (array-like of item ids), unique_users (array-like of user ids)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Make sure types are friendly\n",
    "df_train = df_train.copy()\n",
    "df_train['event'] = df_train['event'].astype('category')\n",
    "# keep categoryid as float if you have NaNs; we'll encode to integer codes below\n",
    "\n",
    "# Ensure indexers\n",
    "unique_items = pd.Index(unique_items)\n",
    "unique_users = pd.Index(unique_users)\n",
    "\n",
    "# 1) Item popularity (sum of weights) — aligned to all unique_items\n",
    "item_pop_train = (\n",
    "    df_train.groupby('itemid', observed=True)['w']\n",
    "    .sum()\n",
    "    .reindex(unique_items, fill_value=0.0)\n",
    "    .astype('float32')\n",
    ")\n",
    "\n",
    "# 2) Item freshness (recency in days) — aligned to all unique_items\n",
    "last_seen = (\n",
    "    df_train.groupby('itemid', observed=True)['event_time']\n",
    "    .max()\n",
    "    .reindex(unique_items)\n",
    ")\n",
    "age_days = (df_train['event_time'].max() - last_seen).dt.days.fillna(999).astype('float32')\n",
    "item_freshness = (1.0 / (1.0 + age_days)).astype('float32')  # 0..1, newer → larger\n",
    "\n",
    "# 3) Category conversion rate (smoothed)\n",
    "train_views = (\n",
    "    df_train.loc[df_train['event'].eq('view')]\n",
    "    .groupby('categoryid', observed=True)['itemid']\n",
    "    .count()\n",
    ")\n",
    "train_txn = (\n",
    "    df_train.loc[df_train['event'].eq('transaction')]\n",
    "    .groupby('categoryid', observed=True)['itemid']\n",
    "    .count()\n",
    ")\n",
    "conv_cat = ((train_txn.add(20, fill_value=0) / train_views.add(100, fill_value=0))).astype('float32')  # Laplace smoothing\n",
    "\n",
    "# 4) Item → category (take the last category seen per item, vectorized)\n",
    "#    Sort once, take groupby-last (fast & avoids Python loops)\n",
    "item_cat = (\n",
    "    df_train[['itemid','categoryid','event_time']]\n",
    "    .sort_values(['itemid','event_time'])\n",
    "    .groupby('itemid', observed=True)['categoryid']\n",
    "    .last()\n",
    "    .reindex(unique_items)\n",
    ")\n",
    "\n",
    "# 5) User–category affinity = share of user’s weighted interactions in each category (vectorized)\n",
    "sum_uc = df_train.groupby(['visitorid','categoryid'], observed=True)['w'].sum()\n",
    "user_cat = (sum_uc / sum_uc.groupby(level=0).transform('sum')).astype('float32')\n",
    "# If you need a plain dict for quick lookups:\n",
    "user_cat_dict = user_cat.to_dict()  # keys: (visitorid, categoryid) → float in [0,1]\n",
    "\n",
    "# ---- Build arrays for constant-time lookups by item index ----\n",
    "\n",
    "# Encode item categories as integer codes for array indexing\n",
    "item_cat_c = pd.Categorical(item_cat)  # categories are sorted unique non-NaN category IDs\n",
    "cat_codes = item_cat_c.codes.astype('int32')        # -1 for NaN\n",
    "cat_values = pd.Index(item_cat_c.categories)        # actual categoryid values\n",
    "\n",
    "# Map conv rate to category-code space\n",
    "conv_per_code = np.zeros(len(cat_values), dtype='float32')\n",
    "if len(cat_values) > 0:\n",
    "    conv_per_code = conv_cat.reindex(cat_values, fill_value=0.0).to_numpy(dtype='float32')\n",
    "\n",
    "# Final per-item arrays (index by item_idx)\n",
    "item_pop_arr    = item_pop_train.to_numpy(dtype='float32')\n",
    "item_fresh_arr  = item_freshness.to_numpy(dtype='float32')\n",
    "item_cat_code   = cat_codes                                   # -1 means \"unknown\"\n",
    "item_conv_arr   = np.where(item_cat_code >= 0, conv_per_code[item_cat_code], 0.0).astype('float32')\n",
    "\n",
    "# Optional: quick helpers that DO NOT touch df_train again\n",
    "def get_item_meta_fast(item_idx: int):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      cat_code (int), pop (float), fresh (float), conv_rate (float)\n",
    "    \"\"\"\n",
    "    c = item_cat_code[item_idx]\n",
    "    pop = float(item_pop_arr[item_idx])\n",
    "    fresh = float(item_fresh_arr[item_idx])\n",
    "    conv = float(item_conv_arr[item_idx])\n",
    "    return c, pop, fresh, conv\n",
    "\n",
    "def get_user_cat_affinity_fast(user_idx: int, cat_code: int):\n",
    "    \"\"\"\n",
    "    cat_code is from item_cat_code. Returns 0.0 if unknown/no affinity.\n",
    "    \"\"\"\n",
    "    if cat_code < 0:\n",
    "        return 0.0\n",
    "    # map back from user_idx → original visitorid, and cat_code → categoryid value\n",
    "    uid = unique_users[user_idx]\n",
    "    catid = cat_values[cat_code]\n",
    "    return float(user_cat_dict.get((uid, catid), 0.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cab0c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes — X_train: (1007109, 203358)  X_BM25: (1007109, 203358)  C_BM25: (1007109, 203358)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import coo_matrix, csr_matrix\n",
    "\n",
    "# 0) Defaults (if not already set)\n",
    "\n",
    "K = globals().get('K', 10)\n",
    "CHUNK_USERS = globals().get('CHUNK_USERS', 20000)\n",
    "CAND_K = globals().get('CAND_K', 200)\n",
    "\n",
    "# Sanity: we expect df_train/df_test with columns at least ['visitorid','itemid','event','event_time']\n",
    "assert {'visitorid','itemid','event'}.issubset(df_train.columns)\n",
    "assert {'visitorid','itemid','event'}.issubset(df_test.columns)\n",
    "\n",
    "# If event weights not present, make them\n",
    "if 'w' not in df_train.columns:\n",
    "    w_map = {'view': 1.0, 'addtocart': 3.0, 'transaction': 5.0}\n",
    "    df_train = df_train.assign(w=df_train['event'].map(w_map).fillna(0.5))\n",
    "\n",
    "\n",
    "# 1) Train-only mappings (critical)\n",
    "\n",
    "users_train = np.unique(df_train['visitorid'].values)\n",
    "items_train = np.unique(df_train['itemid'].values)\n",
    "\n",
    "user2idx = {u: i for i, u in enumerate(users_train)}\n",
    "item2idx = {it: j for j, it in enumerate(items_train)}\n",
    "\n",
    "unique_users = users_train\n",
    "unique_items = items_train\n",
    "\n",
    "\n",
    "# 2) Build CSR interaction matrix (train)\n",
    "\n",
    "row = df_train['visitorid'].map(user2idx).values\n",
    "col = df_train['itemid'].map(item2idx).values\n",
    "dat = df_train['w'].astype(float).values\n",
    "\n",
    "n_users = len(unique_users)\n",
    "n_items = len(unique_items)\n",
    "\n",
    "X_train = coo_matrix((dat, (row, col)), shape=(n_users, n_items)).tocsr()\n",
    "X_train.sum_duplicates()\n",
    "\n",
    "# Seen items per user (for filtering)\n",
    "from collections import defaultdict\n",
    "seen_train = defaultdict(set)\n",
    "for r, c in zip(row, col):\n",
    "    seen_train[r].add(c)\n",
    "\n",
    "\n",
    "# 3) Weighting & safe column L2-normalization\n",
    "\n",
    "def tfidf_weight(X):\n",
    "    # X: csr\n",
    "    Xc = X.tocsc(copy=True)\n",
    "    n_rows, n_cols = Xc.shape\n",
    "    # document frequency (per column nnz)\n",
    "    df_col = np.diff(Xc.indptr)\n",
    "    idf = np.log((n_rows + 1) / (df_col + 1)) + 1.0  # smoothed idf\n",
    "    # tf = log(1 + x)\n",
    "    Xc.data = np.log1p(Xc.data)\n",
    "    Xc.data *= np.repeat(idf, df_col)\n",
    "    return Xc.tocsr()\n",
    "\n",
    "def bm25_weight(X, K1=1.2, B=0.75):\n",
    "    # Work in CSR for per-row length normalization\n",
    "    X = X.tocsr(copy=True)\n",
    "    n_rows, n_cols = X.shape\n",
    "\n",
    "    # idf from column nnz\n",
    "    Xc = X.tocsc()\n",
    "    df_col = np.diff(Xc.indptr)\n",
    "    idf = np.log((n_rows - df_col + 0.5) / (df_col + 0.5))\n",
    "    idf[idf < 0] = 0.0\n",
    "\n",
    "    # row lengths\n",
    "    row_sums = np.asarray(X.sum(axis=1)).ravel()\n",
    "    avgdl = row_sums.mean() + 1e-12\n",
    "    denom_row = K1 * (1 - B + B * (row_sums / avgdl))\n",
    "\n",
    "    # expand row indices per data entry\n",
    "    row_ids = np.repeat(np.arange(n_rows), np.diff(X.indptr))\n",
    "    col_ids = X.indices\n",
    "\n",
    "    tf_prime = (X.data * (K1 + 1.0)) / (X.data + denom_row[row_ids] + 1e-12)\n",
    "    X.data = tf_prime * idf[col_ids]\n",
    "    return X\n",
    "\n",
    "def l2_normalize_columns(X):\n",
    "    # Normalize columns so that cosine(item_i, item_j) works\n",
    "    Xc = X.tocsc(copy=True)\n",
    "    col_norms = np.sqrt(Xc.power(2).sum(axis=0)).A1\n",
    "    col_norms[col_norms == 0] = 1.0\n",
    "    # repeat each column norm for the number of nonzeros in that column\n",
    "    scale = np.repeat(col_norms, np.diff(Xc.indptr))\n",
    "    Xc.data = Xc.data / scale\n",
    "    return Xc.tocsr()\n",
    "\n",
    "# Choose base matrix variant (BM25 typically best here)\n",
    "X_BM25 = bm25_weight(X_train)\n",
    "C_BM25 = l2_normalize_columns(X_BM25)  # same shape (users x items), cols L2-normalized\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Base recommender & toplists (item-cosine via two sparse multiplies)\n",
    "# -----------------------------\n",
    "def recommend_itemcosine(u_idx, X, C, k=K):\n",
    "    \"\"\"\n",
    "    X: users x items CSR (weighted interactions)\n",
    "    C: users x items CSR with L2-normalized columns\n",
    "    Predict scores = (X[u,:] @ C.T) @ C  (implicit item-item cosine)\n",
    "    \"\"\"\n",
    "    if u_idx >= X.shape[0]:\n",
    "        return [], []\n",
    "    w = X[u_idx, :]        # 1 x n_items\n",
    "    if w.nnz == 0:\n",
    "        return [], []\n",
    "\n",
    "    y = (w @ C.T)          # 1 x n_users\n",
    "    scores = (y @ C).toarray().ravel()  # 1 x n_items -> dense row\n",
    "\n",
    "    # filter seen\n",
    "    for it in seen_train.get(u_idx, ()):\n",
    "        scores[it] = -np.inf\n",
    "\n",
    "    if k >= len(scores):\n",
    "        top_idx = np.argsort(-scores)\n",
    "    else:\n",
    "        top_part = np.argpartition(-scores, k)[:k]\n",
    "        top_idx = top_part[np.argsort(-scores[top_part])]\n",
    "    return top_idx[:k].tolist(), scores[top_idx[:k]].tolist()\n",
    "\n",
    "def toplist_itemcosine_bm25(u_idx, k=K):\n",
    "    idx, _ = recommend_itemcosine(u_idx, X_BM25, C_BM25, k)\n",
    "    return idx\n",
    "\n",
    "# Popularity baseline toplist (train popularity)\n",
    "item_pop_train = np.asarray(X_train.sum(axis=0)).ravel()\n",
    "pop_rank = np.argsort(-item_pop_train)\n",
    "\n",
    "def toplist_pop(u_idx, k=K):\n",
    "    # filter seen for this user\n",
    "    seen = seen_train.get(u_idx, set())\n",
    "    if not seen:\n",
    "        return pop_rank[:k].tolist()\n",
    "    # walk down the popularity list skipping seen\n",
    "    out = []\n",
    "    for it in pop_rank:\n",
    "        if it not in seen:\n",
    "            out.append(it)\n",
    "            if len(out) >= k:\n",
    "                break\n",
    "    return out\n",
    "\n",
    "\n",
    "# 5) Test ground-truth (transactions) & eval users aligned to train space\n",
    "\n",
    "test_txn = (\n",
    "    df_test[df_test['event'] == 'transaction']\n",
    "    .groupby('visitorid')['itemid'].apply(set).to_dict()\n",
    ")\n",
    "# users to evaluate: must exist in train mapping\n",
    "eval_users = [u for u in df_test['visitorid'].unique() if u in user2idx]\n",
    "test_truth = test_txn  # alias if your notebook expects this name\n",
    "\n",
    "\n",
    "# 6) Metric helpers (idempotent re-def ok)\n",
    "\n",
    "def recall_at_k(truth_set, rec_list, k):\n",
    "    if not truth_set: return 0.0\n",
    "    hit = len(set(rec_list[:k]) & set(truth_set))\n",
    "    return hit / float(len(truth_set))\n",
    "\n",
    "def ap_at_k(truth_set, rec_list, k):\n",
    "    if not truth_set: return 0.0\n",
    "    score = 0.0\n",
    "    hit = 0\n",
    "    for i, itm in enumerate(rec_list[:k], 1):\n",
    "        if itm in truth_set:\n",
    "            hit += 1\n",
    "            score += hit / i\n",
    "    return score / min(len(truth_set), k) if hit else 0.0\n",
    "\n",
    "def ndcg_at_k(truth_set, rec_list, k):\n",
    "    import math\n",
    "    dcg = 0.0\n",
    "    for i, itm in enumerate(rec_list[:k], 1):\n",
    "        if itm in truth_set:\n",
    "            dcg += 1.0 / math.log2(i + 1)\n",
    "    ideal = 0.0\n",
    "    m = min(len(truth_set), k)\n",
    "    for i in range(1, m + 1):\n",
    "        ideal += 1.0 / math.log2(i + 1)\n",
    "    return dcg / ideal if ideal > 0 else 0.0\n",
    "\n",
    "\n",
    "# 7) Reranker (unchanged; uses metadata you already calculated)\n",
    "\n",
    "def rerank_with_metadata(user_idx, base_items, w_base, w_pop, w_fresh, w_conv, w_aff, rank_decay, k=K):\n",
    "    base_items = np.array(base_items, dtype=np.int64)\n",
    "    if base_items.size == 0: \n",
    "        return np.array([], dtype=np.int64)\n",
    "\n",
    "    # base rank score\n",
    "    rank_pos = np.arange(1, base_items.size + 1, dtype=np.float64)\n",
    "    s_base = 1.0 / (rank_pos ** rank_decay)\n",
    "\n",
    "    # fetch per-item metadata (must exist in your notebook: item_pop_train, item_freshness, conv_cat, user_cat, etc.)\n",
    "    pop = np.array([item_pop_train[unique_items[it]] if unique_items[it] in item_pop_train.index else 0.0\n",
    "                    for it in base_items], dtype=np.float64)\n",
    "    fresh = np.array([item_freshness.get(unique_items[it], 0.0) for it in base_items], dtype=np.float64)\n",
    "\n",
    "    # category & conv rate\n",
    "    cats = []\n",
    "    for it in base_items:\n",
    "        it_orig = unique_items[it]\n",
    "        cat_vals = df_train.loc[df_train['itemid'].eq(it_orig), 'categoryid'].tail(1).values\n",
    "        cats.append(cat_vals[0] if len(cat_vals) else np.nan)\n",
    "    cats = np.array(cats)\n",
    "    conv = np.array([float(conv_cat.get(c, 0.0)) if pd.notna(c) else 0.0 for c in cats], dtype=np.float64)\n",
    "\n",
    "    # user affinity to category\n",
    "    uid_orig = unique_users[user_idx]\n",
    "    def user_cat_aff(c):\n",
    "        if pd.isna(c): return 0.0\n",
    "        return float(user_cat.get((uid_orig, c), 0.0))\n",
    "    aff = np.array([user_cat_aff(c) for c in cats], dtype=np.float64)\n",
    "\n",
    "    # combine\n",
    "    s = (w_base * s_base) + (w_pop * pop) + (w_fresh * fresh) + (w_conv * conv) + (w_aff * aff)\n",
    "    order = np.argsort(-s)\n",
    "    return base_items[order][:k]\n",
    "\n",
    "# -----------------------------\n",
    "# 8) Eval wrapper (baseline vs. hybrid)\n",
    "# -----------------------------\n",
    "def eval_hybrid(weights, label, users_eval=None, chunk_size=CHUNK_USERS, k=K, cand_k=CAND_K):\n",
    "    \"\"\"\n",
    "    weights None => 'no-rerank' baseline using toplist_itemcosine_bm25(u,k)\n",
    "    \"\"\"\n",
    "    if users_eval is None:\n",
    "        users_eval = [u for u in eval_users if u in user2idx]\n",
    "\n",
    "    recalls, maps, ndcgs = [], [], []\n",
    "    n_users = len(users_eval)\n",
    "\n",
    "    for start in range(0, n_users, chunk_size):\n",
    "        batch = users_eval[start:start+chunk_size]\n",
    "        for u_orig in batch:\n",
    "            u_idx = user2idx[u_orig]\n",
    "\n",
    "            truth_items = test_truth.get(u_orig, set())\n",
    "            truth_idx = { item2idx[it] for it in truth_items if it in item2idx }\n",
    "            if not truth_idx:\n",
    "                continue\n",
    "\n",
    "            if weights is None:\n",
    "                rec_idx = toplist_itemcosine_bm25(u_idx, k)\n",
    "            else:\n",
    "                w_base, w_pop, w_fresh, w_conv, w_aff, rd = weights\n",
    "                base_idx = toplist_itemcosine_bm25(u_idx, cand_k)\n",
    "                if not base_idx:\n",
    "                    continue\n",
    "                rec_idx = rerank_with_metadata(\n",
    "                    user_idx=u_idx,\n",
    "                    base_items=base_idx,\n",
    "                    w_base=w_base, w_pop=w_pop, w_fresh=w_fresh, w_conv=w_conv, w_aff=w_aff,\n",
    "                    rank_decay=rd, k=k\n",
    "                ).tolist()\n",
    "\n",
    "            if not rec_idx:\n",
    "                continue\n",
    "\n",
    "            recalls.append(recall_at_k(truth_idx, rec_idx, k))\n",
    "            maps.append(ap_at_k(truth_idx, rec_idx, k))\n",
    "            ndcgs.append(ndcg_at_k(truth_idx, rec_idx, k))\n",
    "\n",
    "    return {\n",
    "        \"label\": label,\n",
    "        \"users\": len(recalls),\n",
    "        \"recall\": float(np.mean(recalls)) if recalls else 0.0,\n",
    "        \"map\":    float(np.mean(maps))    if maps    else 0.0,\n",
    "        \"ndcg\":   float(np.mean(ndcgs))   if ndcgs   else 0.0,\n",
    "    }\n",
    "\n",
    "print(\"Shapes — X_train:\", X_train.shape, \" X_BM25:\", X_BM25.shape, \" C_BM25:\", C_BM25.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3555c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1) Popularity array (by item index) + normalized variant for mixing ---\n",
    "item_pop_arr = np.asarray(X_train.sum(axis=0)).ravel()\n",
    "if item_pop_arr.max() > 0:\n",
    "    pop_norm = np.log1p(item_pop_arr) / np.log1p(item_pop_arr.max())\n",
    "else:\n",
    "    pop_norm = item_pop_arr.copy()  # all zeros edge case\n",
    "\n",
    "# Keep your existing popularity ranking for the baseline toplist\n",
    "pop_rank = np.argsort(-item_pop_arr)\n",
    "\n",
    "# --- 2) Fast item -> category map (last seen in train) ---\n",
    "item_to_cat = (\n",
    "    df_train.sort_values('event_time')\n",
    "            .groupby('itemid')['categoryid']\n",
    "            .last()\n",
    "            .to_dict()\n",
    ")\n",
    "\n",
    "# --- 3) (Optional) convert user_cat to a plain dict for faster .get() ---\n",
    "# user_cat was defined earlier as a normalized share per user/category\n",
    "user_cat_dict = user_cat.to_dict() if hasattr(user_cat, \"to_dict\") else user_cat\n",
    "\n",
    "# --- 4) Replace your reranker to use arrays/dicts (no .index checks) ---\n",
    "def rerank_with_metadata(user_idx, base_items, w_base, w_pop, w_fresh, w_conv, w_aff, rank_decay, k=K):\n",
    "    \"\"\"\n",
    "    base_items: array/list of item *indices* (cols), length = CAND_K\n",
    "    Returns top-k item indices after re-ranking with a simple linear score.\n",
    "    \"\"\"\n",
    "    base_items = np.asarray(base_items, dtype=np.int64)\n",
    "    if base_items.size == 0:\n",
    "        return np.array([], dtype=np.int64)\n",
    "\n",
    "    # base rank decay (higher rank -> higher score)\n",
    "    rank_pos = np.arange(1, base_items.size + 1, dtype=np.float64)\n",
    "    s_base = 1.0 / (rank_pos ** rank_decay)\n",
    "\n",
    "    # --- popularity component (array, indexed by item index) ---\n",
    "    pop = pop_norm[base_items]  # already 0..1\n",
    "\n",
    "    # --- freshness component (map by original item id -> 0..1) ---\n",
    "    it_orig = unique_items[base_items]  # original item ids for these indices\n",
    "    fresh = np.array([float(item_freshness.get(it, 0.0)) for it in it_orig], dtype=np.float64)\n",
    "\n",
    "    # --- category conversion rate (by category id) ---\n",
    "    cats = np.array([item_to_cat.get(it, np.nan) for it in it_orig], dtype='float64')\n",
    "    conv = np.array([float(conv_cat.get(c, 0.0)) if pd.notna(c) else 0.0 for c in cats], dtype=np.float64)\n",
    "\n",
    "    # --- user-category affinity ---\n",
    "    u_orig = unique_users[user_idx]\n",
    "    def u_aff(c):\n",
    "        if pd.isna(c): \n",
    "            return 0.0\n",
    "        return float(user_cat_dict.get((u_orig, c), 0.0))\n",
    "    aff = np.array([u_aff(c) for c in cats], dtype=np.float64)\n",
    "\n",
    "    # --- linear blend ---\n",
    "    s = (w_base * s_base) + (w_pop * pop) + (w_fresh * fresh) + (w_conv * conv) + (w_aff * aff)\n",
    "\n",
    "    order = np.argsort(-s)\n",
    "    return base_items[order][:k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "165235cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>users</th>\n",
       "      <th>recall</th>\n",
       "      <th>map</th>\n",
       "      <th>ndcg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hybrid@10 wb=0.8 wp=0.3 wf=0.3 wc=0.2 wa=0.5 r...</td>\n",
       "      <td>248</td>\n",
       "      <td>0.029779</td>\n",
       "      <td>0.012739</td>\n",
       "      <td>0.018005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hybrid@10 wb=0.9 wp=0.2 wf=0.2 wc=0.2 wa=0.4 r...</td>\n",
       "      <td>248</td>\n",
       "      <td>0.028973</td>\n",
       "      <td>0.010122</td>\n",
       "      <td>0.015444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hybrid@10 wb=0.7 wp=0.4 wf=0.2 wc=0.2 wa=0.5 r...</td>\n",
       "      <td>248</td>\n",
       "      <td>0.028973</td>\n",
       "      <td>0.012477</td>\n",
       "      <td>0.017472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ItemCosine-BM25@10 (no-rerank)</td>\n",
       "      <td>248</td>\n",
       "      <td>0.012769</td>\n",
       "      <td>0.006448</td>\n",
       "      <td>0.008415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               label  users    recall  \\\n",
       "0  Hybrid@10 wb=0.8 wp=0.3 wf=0.3 wc=0.2 wa=0.5 r...    248  0.029779   \n",
       "1  Hybrid@10 wb=0.9 wp=0.2 wf=0.2 wc=0.2 wa=0.4 r...    248  0.028973   \n",
       "2  Hybrid@10 wb=0.7 wp=0.4 wf=0.2 wc=0.2 wa=0.5 r...    248  0.028973   \n",
       "3                     ItemCosine-BM25@10 (no-rerank)    248  0.012769   \n",
       "\n",
       "        map      ndcg  \n",
       "0  0.012739  0.018005  \n",
       "1  0.010122  0.015444  \n",
       "2  0.012477  0.017472  \n",
       "3  0.006448  0.008415  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline: no rerank\n",
    "rows = [eval_hybrid(None, label=\"ItemCosine-BM25@10 (no-rerank)\")]\n",
    "\n",
    "# A tiny weight grid (adjust as you like)\n",
    "weight_grid = [\n",
    "    (0.9, 0.2, 0.2, 0.2, 0.4, 0.7),\n",
    "    (0.8, 0.3, 0.3, 0.2, 0.5, 0.6),\n",
    "    (0.7, 0.4, 0.2, 0.2, 0.5, 0.5),\n",
    "]\n",
    "\n",
    "for w in weight_grid:\n",
    "    lbl = f\"Hybrid@10 wb={w[0]} wp={w[1]} wf={w[2]} wc={w[3]} wa={w[4]} rd={w[5]}\"\n",
    "    rows.append(eval_hybrid(w, label=lbl))\n",
    "\n",
    "eval_hybrid_df = pd.DataFrame(rows).sort_values('recall', ascending=False).reset_index(drop=True)\n",
    "eval_hybrid_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8175fd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Key Insights \n",
    "\n",
    "- **Personalization > Popularity.** At K=10, the personalized **Item–Item BM25** model vastly outperforms the Popularity baseline  \n",
    "  *(Popularity: Recall≈0.0017 / NDCG≈0.0007 → Item–Item BM25: Recall≈0.0775 / NDCG≈0.0418).*\n",
    "\n",
    "- **Best candidate generator:** **Item–Item BM25** is the strongest of the item–item variants; **TF-IDF** and **RAW** trails it, and **SVD(BM25)** is weakest on its own (still useful as a blended signal).\n",
    "\n",
    "- **Hybrid re-ranking lifts quality.** Adding simple business signals (popularity, freshness/age, user–category affinity) on top of base scores **roughly doubles NDCG@10** on the re-rank evaluation cohort *(≈0.0084 → ≈0.0180).*\n",
    "\n",
    "- **Recency matters in practice.** The time-decay (HALF_LIFE≈21 days) and freshness weighting contribute to the observed re-rank gains, indicating recent behavior is more predictive.\n",
    "\n",
    "- **Scope:** Results are computed for **returning users with history**. True cold-start users are **not evaluated** here; a **Trending/Popular** fallback is appropriate until interactions are observed.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
